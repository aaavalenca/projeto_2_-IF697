{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "AY3I7g9ceUZE",
        "rOPNM-Yol4du"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Projeto 2 de IF697 - Introdução a Ciência de Dados\n",
        "\n",
        "\n",
        "*   Alunos: André Valença e Pedro Basílio\n",
        "*   Professor: Luciano Barbosa\n",
        "*   Documentação: https://github.com/ProfLuciano/cd/blob/gh-pages/slides/projeto2.pdf \n",
        "\n"
      ],
      "metadata": {
        "id": "d1_TJMvI7jce"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Organizando os dados\n",
        "\n",
        "Selecionando o dataframe, removendo as colunas desnecessárias, categorizando um dado, fazendo o one hot encoding de outro."
      ],
      "metadata": {
        "id": "AY3I7g9ceUZE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vi49euuRbcsb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = pd.read_csv(\"https://raw.githubusercontent.com/aaavalenca/projeto_2_-IF697/main/mobilidados.csv\")"
      ],
      "metadata": {
        "id": "92kqTx5J7ZUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l.head()"
      ],
      "metadata": {
        "id": "ZMC6hYS0b23Q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "5676957a-c230-43c6-bd9c-89ed487c81a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   CD_UF  UF  CD_MUN6   CD_MUN     MUNICIPIO CAPITAL REGIAO METROPOLITANA  \\\n",
              "0     12  AC   120001  1200013    Acrelândia     Não                    -   \n",
              "1     12  AC   120005  1200054  Assis Brasil     Não                    -   \n",
              "2     12  AC   120010  1200104     Brasiléia     Não                    -   \n",
              "3     12  AC   120013  1200138        Bujari     Não                    -   \n",
              "4     12  AC   120017  1200179      Capixaba     Não                    -   \n",
              "\n",
              "   Frota_2001  Frota_2002  Frota_2003  ...  Frota_2012  Frota_2013  \\\n",
              "0         148         254         380  ...        3187        3543   \n",
              "1          44          65          87  ...         795         940   \n",
              "2         683         874        1064  ...        4737        5329   \n",
              "3          34          55         107  ...         993        1146   \n",
              "4          36          65          98  ...        1008        1154   \n",
              "\n",
              "   Frota_2014  Frota_2015  Frota_2016  Frota_2017  Frota_2018  Frota_2019  \\\n",
              "0        3971        4364        4648        4856        5163        5541   \n",
              "1        1119        1271        1363        1485        1615        1700   \n",
              "2        6017        6697        7235        7638        8174        8840   \n",
              "3        1309        1434        1570        1652        1780        1860   \n",
              "4        1287        1468        1635        1748        1890        2003   \n",
              "\n",
              "   Frota_2020  Frota_2021  \n",
              "0        6045        6465  \n",
              "1        1879        2019  \n",
              "2        9450       10280  \n",
              "3        2022        2172  \n",
              "4        2167        2334  \n",
              "\n",
              "[5 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-07541b99-8fb7-4fc4-ab3f-37c580c74ce9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CD_UF</th>\n",
              "      <th>UF</th>\n",
              "      <th>CD_MUN6</th>\n",
              "      <th>CD_MUN</th>\n",
              "      <th>MUNICIPIO</th>\n",
              "      <th>CAPITAL</th>\n",
              "      <th>REGIAO METROPOLITANA</th>\n",
              "      <th>Frota_2001</th>\n",
              "      <th>Frota_2002</th>\n",
              "      <th>Frota_2003</th>\n",
              "      <th>...</th>\n",
              "      <th>Frota_2012</th>\n",
              "      <th>Frota_2013</th>\n",
              "      <th>Frota_2014</th>\n",
              "      <th>Frota_2015</th>\n",
              "      <th>Frota_2016</th>\n",
              "      <th>Frota_2017</th>\n",
              "      <th>Frota_2018</th>\n",
              "      <th>Frota_2019</th>\n",
              "      <th>Frota_2020</th>\n",
              "      <th>Frota_2021</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>12</td>\n",
              "      <td>AC</td>\n",
              "      <td>120001</td>\n",
              "      <td>1200013</td>\n",
              "      <td>Acrelândia</td>\n",
              "      <td>Não</td>\n",
              "      <td>-</td>\n",
              "      <td>148</td>\n",
              "      <td>254</td>\n",
              "      <td>380</td>\n",
              "      <td>...</td>\n",
              "      <td>3187</td>\n",
              "      <td>3543</td>\n",
              "      <td>3971</td>\n",
              "      <td>4364</td>\n",
              "      <td>4648</td>\n",
              "      <td>4856</td>\n",
              "      <td>5163</td>\n",
              "      <td>5541</td>\n",
              "      <td>6045</td>\n",
              "      <td>6465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>12</td>\n",
              "      <td>AC</td>\n",
              "      <td>120005</td>\n",
              "      <td>1200054</td>\n",
              "      <td>Assis Brasil</td>\n",
              "      <td>Não</td>\n",
              "      <td>-</td>\n",
              "      <td>44</td>\n",
              "      <td>65</td>\n",
              "      <td>87</td>\n",
              "      <td>...</td>\n",
              "      <td>795</td>\n",
              "      <td>940</td>\n",
              "      <td>1119</td>\n",
              "      <td>1271</td>\n",
              "      <td>1363</td>\n",
              "      <td>1485</td>\n",
              "      <td>1615</td>\n",
              "      <td>1700</td>\n",
              "      <td>1879</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>12</td>\n",
              "      <td>AC</td>\n",
              "      <td>120010</td>\n",
              "      <td>1200104</td>\n",
              "      <td>Brasiléia</td>\n",
              "      <td>Não</td>\n",
              "      <td>-</td>\n",
              "      <td>683</td>\n",
              "      <td>874</td>\n",
              "      <td>1064</td>\n",
              "      <td>...</td>\n",
              "      <td>4737</td>\n",
              "      <td>5329</td>\n",
              "      <td>6017</td>\n",
              "      <td>6697</td>\n",
              "      <td>7235</td>\n",
              "      <td>7638</td>\n",
              "      <td>8174</td>\n",
              "      <td>8840</td>\n",
              "      <td>9450</td>\n",
              "      <td>10280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>12</td>\n",
              "      <td>AC</td>\n",
              "      <td>120013</td>\n",
              "      <td>1200138</td>\n",
              "      <td>Bujari</td>\n",
              "      <td>Não</td>\n",
              "      <td>-</td>\n",
              "      <td>34</td>\n",
              "      <td>55</td>\n",
              "      <td>107</td>\n",
              "      <td>...</td>\n",
              "      <td>993</td>\n",
              "      <td>1146</td>\n",
              "      <td>1309</td>\n",
              "      <td>1434</td>\n",
              "      <td>1570</td>\n",
              "      <td>1652</td>\n",
              "      <td>1780</td>\n",
              "      <td>1860</td>\n",
              "      <td>2022</td>\n",
              "      <td>2172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>12</td>\n",
              "      <td>AC</td>\n",
              "      <td>120017</td>\n",
              "      <td>1200179</td>\n",
              "      <td>Capixaba</td>\n",
              "      <td>Não</td>\n",
              "      <td>-</td>\n",
              "      <td>36</td>\n",
              "      <td>65</td>\n",
              "      <td>98</td>\n",
              "      <td>...</td>\n",
              "      <td>1008</td>\n",
              "      <td>1154</td>\n",
              "      <td>1287</td>\n",
              "      <td>1468</td>\n",
              "      <td>1635</td>\n",
              "      <td>1748</td>\n",
              "      <td>1890</td>\n",
              "      <td>2003</td>\n",
              "      <td>2167</td>\n",
              "      <td>2334</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 28 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07541b99-8fb7-4fc4-ab3f-37c580c74ce9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-07541b99-8fb7-4fc4-ab3f-37c580c74ce9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-07541b99-8fb7-4fc4-ab3f-37c580c74ce9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou12_TfIFk5G",
        "outputId": "44d39eb5-14a3-4b83-d8b7-2caf0dc61a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CD_UF                    int64\n",
              "UF                      object\n",
              "CD_MUN6                  int64\n",
              "CD_MUN                   int64\n",
              "MUNICIPIO               object\n",
              "CAPITAL                 object\n",
              "REGIAO METROPOLITANA    object\n",
              "Frota_2001               int64\n",
              "Frota_2002               int64\n",
              "Frota_2003               int64\n",
              "Frota_2004               int64\n",
              "Frota_2005               int64\n",
              "Frota_2006               int64\n",
              "Frota_2007               int64\n",
              "Frota_2008               int64\n",
              "Frota_2009               int64\n",
              "Frota_2010               int64\n",
              "Frota_2011               int64\n",
              "Frota_2012               int64\n",
              "Frota_2013               int64\n",
              "Frota_2014               int64\n",
              "Frota_2015               int64\n",
              "Frota_2016               int64\n",
              "Frota_2017               int64\n",
              "Frota_2018               int64\n",
              "Frota_2019               int64\n",
              "Frota_2020               int64\n",
              "Frota_2021               int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Removendo desnecessários\n",
        "\n",
        "l = l.drop('MUNICIPIO', axis=1).drop('CD_UF', axis=1).drop('CD_MUN6', axis=1).drop('CD_MUN', axis=1).drop('REGIAO METROPOLITANA', axis=1)"
      ],
      "metadata": {
        "id": "6zHcFYGCF10r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "sd_SR0_rJGov",
        "outputId": "8506c358-0856-4b16-9632-1e330996da8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   UF CAPITAL  Frota_2001  Frota_2002  Frota_2003  Frota_2004  Frota_2005  \\\n",
              "0  AC     Não         148         254         380         605         847   \n",
              "1  AC     Não          44          65          87         115         159   \n",
              "2  AC     Não         683         874        1064        1228        1468   \n",
              "3  AC     Não          34          55         107         157         219   \n",
              "4  AC     Não          36          65          98         129         171   \n",
              "\n",
              "   Frota_2006  Frota_2007  Frota_2008  ...  Frota_2012  Frota_2013  \\\n",
              "0        1061        1288        1627  ...        3187        3543   \n",
              "1         236         294         366  ...         795         940   \n",
              "2        1749        2117        2565  ...        4737        5329   \n",
              "3         274         336         441  ...         993        1146   \n",
              "4         239         348         449  ...        1008        1154   \n",
              "\n",
              "   Frota_2014  Frota_2015  Frota_2016  Frota_2017  Frota_2018  Frota_2019  \\\n",
              "0        3971        4364        4648        4856        5163        5541   \n",
              "1        1119        1271        1363        1485        1615        1700   \n",
              "2        6017        6697        7235        7638        8174        8840   \n",
              "3        1309        1434        1570        1652        1780        1860   \n",
              "4        1287        1468        1635        1748        1890        2003   \n",
              "\n",
              "   Frota_2020  Frota_2021  \n",
              "0        6045        6465  \n",
              "1        1879        2019  \n",
              "2        9450       10280  \n",
              "3        2022        2172  \n",
              "4        2167        2334  \n",
              "\n",
              "[5 rows x 23 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5d143d23-5fc8-4425-a3bd-531c81e43f3b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>UF</th>\n",
              "      <th>CAPITAL</th>\n",
              "      <th>Frota_2001</th>\n",
              "      <th>Frota_2002</th>\n",
              "      <th>Frota_2003</th>\n",
              "      <th>Frota_2004</th>\n",
              "      <th>Frota_2005</th>\n",
              "      <th>Frota_2006</th>\n",
              "      <th>Frota_2007</th>\n",
              "      <th>Frota_2008</th>\n",
              "      <th>...</th>\n",
              "      <th>Frota_2012</th>\n",
              "      <th>Frota_2013</th>\n",
              "      <th>Frota_2014</th>\n",
              "      <th>Frota_2015</th>\n",
              "      <th>Frota_2016</th>\n",
              "      <th>Frota_2017</th>\n",
              "      <th>Frota_2018</th>\n",
              "      <th>Frota_2019</th>\n",
              "      <th>Frota_2020</th>\n",
              "      <th>Frota_2021</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AC</td>\n",
              "      <td>Não</td>\n",
              "      <td>148</td>\n",
              "      <td>254</td>\n",
              "      <td>380</td>\n",
              "      <td>605</td>\n",
              "      <td>847</td>\n",
              "      <td>1061</td>\n",
              "      <td>1288</td>\n",
              "      <td>1627</td>\n",
              "      <td>...</td>\n",
              "      <td>3187</td>\n",
              "      <td>3543</td>\n",
              "      <td>3971</td>\n",
              "      <td>4364</td>\n",
              "      <td>4648</td>\n",
              "      <td>4856</td>\n",
              "      <td>5163</td>\n",
              "      <td>5541</td>\n",
              "      <td>6045</td>\n",
              "      <td>6465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AC</td>\n",
              "      <td>Não</td>\n",
              "      <td>44</td>\n",
              "      <td>65</td>\n",
              "      <td>87</td>\n",
              "      <td>115</td>\n",
              "      <td>159</td>\n",
              "      <td>236</td>\n",
              "      <td>294</td>\n",
              "      <td>366</td>\n",
              "      <td>...</td>\n",
              "      <td>795</td>\n",
              "      <td>940</td>\n",
              "      <td>1119</td>\n",
              "      <td>1271</td>\n",
              "      <td>1363</td>\n",
              "      <td>1485</td>\n",
              "      <td>1615</td>\n",
              "      <td>1700</td>\n",
              "      <td>1879</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AC</td>\n",
              "      <td>Não</td>\n",
              "      <td>683</td>\n",
              "      <td>874</td>\n",
              "      <td>1064</td>\n",
              "      <td>1228</td>\n",
              "      <td>1468</td>\n",
              "      <td>1749</td>\n",
              "      <td>2117</td>\n",
              "      <td>2565</td>\n",
              "      <td>...</td>\n",
              "      <td>4737</td>\n",
              "      <td>5329</td>\n",
              "      <td>6017</td>\n",
              "      <td>6697</td>\n",
              "      <td>7235</td>\n",
              "      <td>7638</td>\n",
              "      <td>8174</td>\n",
              "      <td>8840</td>\n",
              "      <td>9450</td>\n",
              "      <td>10280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AC</td>\n",
              "      <td>Não</td>\n",
              "      <td>34</td>\n",
              "      <td>55</td>\n",
              "      <td>107</td>\n",
              "      <td>157</td>\n",
              "      <td>219</td>\n",
              "      <td>274</td>\n",
              "      <td>336</td>\n",
              "      <td>441</td>\n",
              "      <td>...</td>\n",
              "      <td>993</td>\n",
              "      <td>1146</td>\n",
              "      <td>1309</td>\n",
              "      <td>1434</td>\n",
              "      <td>1570</td>\n",
              "      <td>1652</td>\n",
              "      <td>1780</td>\n",
              "      <td>1860</td>\n",
              "      <td>2022</td>\n",
              "      <td>2172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AC</td>\n",
              "      <td>Não</td>\n",
              "      <td>36</td>\n",
              "      <td>65</td>\n",
              "      <td>98</td>\n",
              "      <td>129</td>\n",
              "      <td>171</td>\n",
              "      <td>239</td>\n",
              "      <td>348</td>\n",
              "      <td>449</td>\n",
              "      <td>...</td>\n",
              "      <td>1008</td>\n",
              "      <td>1154</td>\n",
              "      <td>1287</td>\n",
              "      <td>1468</td>\n",
              "      <td>1635</td>\n",
              "      <td>1748</td>\n",
              "      <td>1890</td>\n",
              "      <td>2003</td>\n",
              "      <td>2167</td>\n",
              "      <td>2334</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 23 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d143d23-5fc8-4425-a3bd-531c81e43f3b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5d143d23-5fc8-4425-a3bd-531c81e43f3b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5d143d23-5fc8-4425-a3bd-531c81e43f3b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l.isnull().values.any()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA1xcYN2NRQw",
        "outputId": "ea4656d5-2992-40b8-ed02-57d119d238b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "cBGI-BV7Nibb",
        "outputId": "9b1ba90c-f062-47a1-ddb3-926d526c2981"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Frota_2001    Frota_2002    Frota_2003    Frota_2004    Frota_2005  \\\n",
              "count  5.570000e+03  5.570000e+03  5.570000e+03  5.570000e+03  5.570000e+03   \n",
              "mean   5.223276e+03  5.618993e+03  6.015244e+03  6.444061e+03  6.920718e+03   \n",
              "std    5.868858e+04  6.161065e+04  6.422297e+04  6.679588e+04  6.999387e+04   \n",
              "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "25%    1.822500e+02  2.210000e+02  2.660000e+02  3.162500e+02  3.642500e+02   \n",
              "50%    6.155000e+02  6.900000e+02  7.820000e+02  8.875000e+02  9.780000e+02   \n",
              "75%    1.927500e+03  2.126250e+03  2.342750e+03  2.591750e+03  2.824500e+03   \n",
              "max    3.796711e+06  3.977491e+06  4.139511e+06  4.296386e+06  4.494597e+06   \n",
              "\n",
              "         Frota_2006    Frota_2007    Frota_2008    Frota_2009    Frota_2010  \\\n",
              "count  5.570000e+03  5.570000e+03  5.570000e+03  5.570000e+03  5.570000e+03   \n",
              "mean   7.480798e+03  8.206751e+03  9.029427e+03  9.854898e+03  1.077149e+04   \n",
              "std    7.427024e+04  7.969464e+04  8.587978e+04  9.137130e+04  9.608481e+04   \n",
              "min    0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
              "25%    4.350000e+02  5.042500e+02  5.892500e+02  6.790000e+02  7.812500e+02   \n",
              "50%    1.088500e+03  1.229000e+03  1.388000e+03  1.564500e+03  1.784500e+03   \n",
              "75%    3.072500e+03  3.441750e+03  3.871500e+03  4.340750e+03  4.772500e+03   \n",
              "max    4.771165e+06  5.114957e+06  5.513047e+06  5.845428e+06  6.095515e+06   \n",
              "\n",
              "       ...    Frota_2012    Frota_2013    Frota_2014    Frota_2015  \\\n",
              "count  ...  5.570000e+03  5.570000e+03  5.570000e+03  5.570000e+03   \n",
              "mean   ...  1.266629e+04  1.357400e+04  1.442221e+04  1.508192e+04   \n",
              "std    ...  1.044963e+05  1.086794e+05  1.136633e+05  1.176497e+05   \n",
              "min    ...  0.000000e+00  1.000000e+01  1.000000e+01  1.000000e+01   \n",
              "25%    ...  1.007250e+03  1.126250e+03  1.235250e+03  1.327500e+03   \n",
              "50%    ...  2.227500e+03  2.470500e+03  2.689000e+03  2.884500e+03   \n",
              "75%    ...  5.944500e+03  6.549250e+03  7.101750e+03  7.518500e+03   \n",
              "max    ...  6.501475e+06  6.714695e+06  7.015043e+06  7.270780e+06   \n",
              "\n",
              "         Frota_2016    Frota_2017    Frota_2018    Frota_2019    Frota_2020  \\\n",
              "count  5.570000e+03  5.570000e+03  5.570000e+03  5.570000e+03  5.570000e+03   \n",
              "mean   1.559630e+04  1.613367e+04  1.674165e+04  1.740750e+04  1.791870e+04   \n",
              "std    1.208289e+05  1.243728e+05  1.285033e+05  1.330042e+05  1.357134e+05   \n",
              "min    1.100000e+01  9.000000e+00  8.000000e+00  6.000000e+00  7.000000e+00   \n",
              "25%    1.404000e+03  1.471250e+03  1.542000e+03  1.610250e+03  1.693250e+03   \n",
              "50%    3.035000e+03  3.179500e+03  3.316000e+03  3.456500e+03  3.605500e+03   \n",
              "75%    7.903000e+03  8.255000e+03  8.606500e+03  9.020000e+03  9.424750e+03   \n",
              "max    7.475969e+06  7.700558e+06  7.950995e+06  8.209314e+06  8.391816e+06   \n",
              "\n",
              "         Frota_2021  \n",
              "count  5.570000e+03  \n",
              "mean   1.847674e+04  \n",
              "std    1.380668e+05  \n",
              "min    8.000000e+00  \n",
              "25%    1.783750e+03  \n",
              "50%    3.787000e+03  \n",
              "75%    9.788500e+03  \n",
              "max    8.507507e+06  \n",
              "\n",
              "[8 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-36bf0375-b9d6-4fc4-b495-a9d44df97636\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frota_2001</th>\n",
              "      <th>Frota_2002</th>\n",
              "      <th>Frota_2003</th>\n",
              "      <th>Frota_2004</th>\n",
              "      <th>Frota_2005</th>\n",
              "      <th>Frota_2006</th>\n",
              "      <th>Frota_2007</th>\n",
              "      <th>Frota_2008</th>\n",
              "      <th>Frota_2009</th>\n",
              "      <th>Frota_2010</th>\n",
              "      <th>...</th>\n",
              "      <th>Frota_2012</th>\n",
              "      <th>Frota_2013</th>\n",
              "      <th>Frota_2014</th>\n",
              "      <th>Frota_2015</th>\n",
              "      <th>Frota_2016</th>\n",
              "      <th>Frota_2017</th>\n",
              "      <th>Frota_2018</th>\n",
              "      <th>Frota_2019</th>\n",
              "      <th>Frota_2020</th>\n",
              "      <th>Frota_2021</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>...</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "      <td>5.570000e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>5.223276e+03</td>\n",
              "      <td>5.618993e+03</td>\n",
              "      <td>6.015244e+03</td>\n",
              "      <td>6.444061e+03</td>\n",
              "      <td>6.920718e+03</td>\n",
              "      <td>7.480798e+03</td>\n",
              "      <td>8.206751e+03</td>\n",
              "      <td>9.029427e+03</td>\n",
              "      <td>9.854898e+03</td>\n",
              "      <td>1.077149e+04</td>\n",
              "      <td>...</td>\n",
              "      <td>1.266629e+04</td>\n",
              "      <td>1.357400e+04</td>\n",
              "      <td>1.442221e+04</td>\n",
              "      <td>1.508192e+04</td>\n",
              "      <td>1.559630e+04</td>\n",
              "      <td>1.613367e+04</td>\n",
              "      <td>1.674165e+04</td>\n",
              "      <td>1.740750e+04</td>\n",
              "      <td>1.791870e+04</td>\n",
              "      <td>1.847674e+04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>5.868858e+04</td>\n",
              "      <td>6.161065e+04</td>\n",
              "      <td>6.422297e+04</td>\n",
              "      <td>6.679588e+04</td>\n",
              "      <td>6.999387e+04</td>\n",
              "      <td>7.427024e+04</td>\n",
              "      <td>7.969464e+04</td>\n",
              "      <td>8.587978e+04</td>\n",
              "      <td>9.137130e+04</td>\n",
              "      <td>9.608481e+04</td>\n",
              "      <td>...</td>\n",
              "      <td>1.044963e+05</td>\n",
              "      <td>1.086794e+05</td>\n",
              "      <td>1.136633e+05</td>\n",
              "      <td>1.176497e+05</td>\n",
              "      <td>1.208289e+05</td>\n",
              "      <td>1.243728e+05</td>\n",
              "      <td>1.285033e+05</td>\n",
              "      <td>1.330042e+05</td>\n",
              "      <td>1.357134e+05</td>\n",
              "      <td>1.380668e+05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>1.000000e+01</td>\n",
              "      <td>1.100000e+01</td>\n",
              "      <td>9.000000e+00</td>\n",
              "      <td>8.000000e+00</td>\n",
              "      <td>6.000000e+00</td>\n",
              "      <td>7.000000e+00</td>\n",
              "      <td>8.000000e+00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.822500e+02</td>\n",
              "      <td>2.210000e+02</td>\n",
              "      <td>2.660000e+02</td>\n",
              "      <td>3.162500e+02</td>\n",
              "      <td>3.642500e+02</td>\n",
              "      <td>4.350000e+02</td>\n",
              "      <td>5.042500e+02</td>\n",
              "      <td>5.892500e+02</td>\n",
              "      <td>6.790000e+02</td>\n",
              "      <td>7.812500e+02</td>\n",
              "      <td>...</td>\n",
              "      <td>1.007250e+03</td>\n",
              "      <td>1.126250e+03</td>\n",
              "      <td>1.235250e+03</td>\n",
              "      <td>1.327500e+03</td>\n",
              "      <td>1.404000e+03</td>\n",
              "      <td>1.471250e+03</td>\n",
              "      <td>1.542000e+03</td>\n",
              "      <td>1.610250e+03</td>\n",
              "      <td>1.693250e+03</td>\n",
              "      <td>1.783750e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>6.155000e+02</td>\n",
              "      <td>6.900000e+02</td>\n",
              "      <td>7.820000e+02</td>\n",
              "      <td>8.875000e+02</td>\n",
              "      <td>9.780000e+02</td>\n",
              "      <td>1.088500e+03</td>\n",
              "      <td>1.229000e+03</td>\n",
              "      <td>1.388000e+03</td>\n",
              "      <td>1.564500e+03</td>\n",
              "      <td>1.784500e+03</td>\n",
              "      <td>...</td>\n",
              "      <td>2.227500e+03</td>\n",
              "      <td>2.470500e+03</td>\n",
              "      <td>2.689000e+03</td>\n",
              "      <td>2.884500e+03</td>\n",
              "      <td>3.035000e+03</td>\n",
              "      <td>3.179500e+03</td>\n",
              "      <td>3.316000e+03</td>\n",
              "      <td>3.456500e+03</td>\n",
              "      <td>3.605500e+03</td>\n",
              "      <td>3.787000e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.927500e+03</td>\n",
              "      <td>2.126250e+03</td>\n",
              "      <td>2.342750e+03</td>\n",
              "      <td>2.591750e+03</td>\n",
              "      <td>2.824500e+03</td>\n",
              "      <td>3.072500e+03</td>\n",
              "      <td>3.441750e+03</td>\n",
              "      <td>3.871500e+03</td>\n",
              "      <td>4.340750e+03</td>\n",
              "      <td>4.772500e+03</td>\n",
              "      <td>...</td>\n",
              "      <td>5.944500e+03</td>\n",
              "      <td>6.549250e+03</td>\n",
              "      <td>7.101750e+03</td>\n",
              "      <td>7.518500e+03</td>\n",
              "      <td>7.903000e+03</td>\n",
              "      <td>8.255000e+03</td>\n",
              "      <td>8.606500e+03</td>\n",
              "      <td>9.020000e+03</td>\n",
              "      <td>9.424750e+03</td>\n",
              "      <td>9.788500e+03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.796711e+06</td>\n",
              "      <td>3.977491e+06</td>\n",
              "      <td>4.139511e+06</td>\n",
              "      <td>4.296386e+06</td>\n",
              "      <td>4.494597e+06</td>\n",
              "      <td>4.771165e+06</td>\n",
              "      <td>5.114957e+06</td>\n",
              "      <td>5.513047e+06</td>\n",
              "      <td>5.845428e+06</td>\n",
              "      <td>6.095515e+06</td>\n",
              "      <td>...</td>\n",
              "      <td>6.501475e+06</td>\n",
              "      <td>6.714695e+06</td>\n",
              "      <td>7.015043e+06</td>\n",
              "      <td>7.270780e+06</td>\n",
              "      <td>7.475969e+06</td>\n",
              "      <td>7.700558e+06</td>\n",
              "      <td>7.950995e+06</td>\n",
              "      <td>8.209314e+06</td>\n",
              "      <td>8.391816e+06</td>\n",
              "      <td>8.507507e+06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36bf0375-b9d6-4fc4-b495-a9d44df97636')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-36bf0375-b9d6-4fc4-b495-a9d44df97636 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-36bf0375-b9d6-4fc4-b495-a9d44df97636');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l[\"UF\"].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdc1edf6Dzvx",
        "outputId": "69227044-7ec8-4f2a-e19b-f3b6f36fe094"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['AC', 'AL', 'AM', 'AP', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', 'MG',\n",
              "       'MS', 'MT', 'PA', 'PB', 'PE', 'PI', 'PR', 'RJ', 'RN', 'RO', 'RR',\n",
              "       'RS', 'SC', 'SE', 'SP', 'TO'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l[\"CAPITAL\"].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkYTnZqfO8xK",
        "outputId": "560c00ef-2a1e-4d53-a1fa-b79406919532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Não', 'Sim'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Os dados são distribuídos logaritmicamente, por conta do tamanho das cidades\n",
        "\n",
        "l[\"Frota_2021\"].apply(np.log10).hist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "Zb5hfbw4ccNX",
        "outputId": "6e89b3c2-35b0-4e03-ccbe-7e32d42da5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5fd8da79d0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One hot encoding\n",
        "\n",
        "Não vamos usar de fato, pois usaremos apenas os dados numéricos, mas vamos fazer um processo aqui apenas para demonstrar."
      ],
      "metadata": {
        "id": "WBfUANxilIUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vendo quais são as com maior frota atualmente\n",
        "\n",
        "l.groupby(\"UF\").sum().sort_values(\"Frota_2021\", ascending=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U0BIraLQWSE-",
        "outputId": "232b5eb8-b03d-4361-f49c-76fe31d35280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Frota_2001  Frota_2002  Frota_2003  Frota_2004  Frota_2005  Frota_2006  \\\n",
              "UF                                                                           \n",
              "SP    10479722    11120530    11726049    12387248    13156526    14119987   \n",
              "MG     3095800     3304116     3532779     3762891     4037389     4382741   \n",
              "PR     2265129     2439038     2635359     2873201     3108955     3344715   \n",
              "RS     2420781     2584207     2761576     2948366     3123273     3306810   \n",
              "RJ     2420433     2589084     2723852     2857823     3003923     3170707   \n",
              "SC     1425531     1557072     1697672     1855680     2030201     2215862   \n",
              "BA      795140      883017      973975     1064157     1173092     1297258   \n",
              "GO      926122     1011642     1098583     1193607     1297352     1417705   \n",
              "CE      647275      711101      772022      831054      898707      988200   \n",
              "PE      724389      788089      842213      900404      965805     1048077   \n",
              "PA      279713      313054      353391      397715      448652      501158   \n",
              "MT      364758      413019      465387      527584      586913      639597   \n",
              "ES      483331      524359      564959      613695      668692      737878   \n",
              "DF      610273      651501      693136      736805      782578      845577   \n",
              "MA      207170      231810      261459      294018      336706      384945   \n",
              "MS      386848      425638      462951      505641      550667      596871   \n",
              "PB      251149      275795      301453      325359      354174      393191   \n",
              "RN      255075      288605      314915      345157      380123      425175   \n",
              "PI      170444      194811      220234      246278      275378      312579   \n",
              "RO      170331      190842      212130      239601      267639      298040   \n",
              "AM      183930      203137      223124      248669      280997      312597   \n",
              "AL      164779      179502      196505      212548      231011      252801   \n",
              "SE      160162      176127      192342      207787      224057      243795   \n",
              "TO       92676      112953      133909      156266      184066      225052   \n",
              "AC       43151       48474       54356       60739       67967       77072   \n",
              "RR       38975       44850       50553       55557       60583       67949   \n",
              "AP       30558       35419       40026       45570       52973       61704   \n",
              "\n",
              "    Frota_2007  Frota_2008  Frota_2009  Frota_2010  ...  Frota_2012  \\\n",
              "UF                                                  ...               \n",
              "SP    15337079    16652255    17880303    19196525  ...    21786615   \n",
              "MG     4831158     5363478     5879699     6460080  ...     7659691   \n",
              "PR     3661321     4007943     4322782     4663090  ...     5389621   \n",
              "RS     3538276     3810977     4085309     4369744  ...     5020750   \n",
              "RJ     3404576     3669995     3939390     4254169  ...     4936331   \n",
              "SC     2436675     2656860     2886284     3133685  ...     3622811   \n",
              "BA     1453960     1648855     1873149     2124487  ...     2652076   \n",
              "GO     1592651     1797672     1987496     2195945  ...     2635671   \n",
              "CE     1108077     1240437     1404090     1613903  ...     2048665   \n",
              "PE     1158546     1292762     1449215     1643907  ...     2045263   \n",
              "PA      575899      671371      776604      890856  ...     1167985   \n",
              "MT      719262      832873      938475     1048751  ...     1277180   \n",
              "ES      834105      944809     1050233     1141578  ...     1342793   \n",
              "DF      926080     1006954     1096593     1190062  ...     1361430   \n",
              "MA      451681      529524      627324      752323  ...     1020563   \n",
              "MS      658558      728997      800146      882658  ...     1052495   \n",
              "PB      447845      511993      579852      662730  ...      835750   \n",
              "RN      487482      551541      614950      690175  ...      838659   \n",
              "PI      355889      412045      475404      553349  ...      722195   \n",
              "RO      337834      391888      451941      516948  ...      647049   \n",
              "AM      348992      391739      436955      490989  ...      596725   \n",
              "AL      280680      313604      353304      401113  ...      510163   \n",
              "SE      270877      302716      345441      393289  ...      489313   \n",
              "TO      253822      284106      315328      354658  ...      434027   \n",
              "AC       89145      104315      122393      142994  ...      178288   \n",
              "RR       80060       92200      104589      119429  ...      144839   \n",
              "AP       71073       82001       94532      109772  ...      134262   \n",
              "\n",
              "    Frota_2013  Frota_2014  Frota_2015  Frota_2016  Frota_2017  Frota_2018  \\\n",
              "UF                                                                           \n",
              "SP    22984871    24074875    24920390    25609890    26379038    27248432   \n",
              "MG     8204909     8713866     9116225     9473636     9877221    10322672   \n",
              "PR     5746038     6077970     6298966     6461473     6637325     6856556   \n",
              "RS     5359751     5664159     5883535     6058902     6242464     6448570   \n",
              "RJ     5270462     5594401     5839571     6013457     6165834     6342302   \n",
              "SC     3860577     4083148     4246772     4383715     4543946     4730433   \n",
              "BA     2912176     3163222     3365071     3516255     3662126     3823653   \n",
              "GO     2842455     3028799     3165076     3261283     3362996     3483471   \n",
              "CE     2256965     2458806     2632028     2752124     2858928     2975118   \n",
              "PE     2226463     2387299     2505332     2585182     2669558     2766024   \n",
              "PA     1320954     1468345     1596959     1694592     1779613     1869823   \n",
              "MT     1393932     1509614     1600270     1678056     1753049     1852456   \n",
              "ES     1437215     1528510     1595033     1644748     1697352     1759414   \n",
              "DF     1447068     1518361     1578644     1626709     1677003     1735359   \n",
              "MA     1149680     1272622     1381272     1458747     1532211     1605403   \n",
              "MS     1138186     1215773     1276071     1322279     1373073     1429496   \n",
              "PB      911963      985639     1052095     1104083     1153643     1207180   \n",
              "RN      912976      984481     1045065     1093746     1142962     1191130   \n",
              "PI      810963      898204      971345     1028808     1081116     1132902   \n",
              "RO      701403      755698      800807      835837      868641      909027   \n",
              "AM      651120      703733      741684      765409      792599      827751   \n",
              "AL      564781      618177      661096      694703      730176      767520   \n",
              "SE      530907      573974      609505      635716      662561      690040   \n",
              "TO      474648      514457      550332      573433      595586      620909   \n",
              "AC      194603      211329      226569      237604      249261      262595   \n",
              "RR      157285      170007      181398      190774      199510      207729   \n",
              "AP      144815      156261      165206      170255      176746      185038   \n",
              "\n",
              "    Frota_2019  Frota_2020  Frota_2021  \n",
              "UF                                      \n",
              "SP    28185930    28847315    29452035  \n",
              "MG    10818192    11102097    11508252  \n",
              "PR     7107117     7314208     7542118  \n",
              "RS     6656528     6822109     6987084  \n",
              "RJ     6557189     6688780     6886291  \n",
              "SC     4936317     5108671     5278406  \n",
              "BA     4001194     4159317     4328600  \n",
              "GO     3609048     3731384     3868162  \n",
              "CE     3102748     3209041     3319156  \n",
              "PE     2876606     2963364     3065027  \n",
              "PA     1968561     2070053     2182858  \n",
              "MT     1960549     2053994     2152577  \n",
              "ES     1826559     1887361     1960489  \n",
              "DF     1804081     1851271     1894859  \n",
              "MA     1680568     1754277     1832383  \n",
              "MS     1488085     1536874     1583331  \n",
              "PB     1263944     1316572     1373274  \n",
              "RN     1241174     1281267     1329223  \n",
              "PI     1183315     1221031     1265094  \n",
              "RO      951167      987460     1025606  \n",
              "AM      871225      910285      951120  \n",
              "AL      812919      849278      894965  \n",
              "SE      720170      746440      777144  \n",
              "TO      648467      675708      706998  \n",
              "AC      276579      289009      301858  \n",
              "RR      216525      225580      235813  \n",
              "AP      195012      204414      212706  \n",
              "\n",
              "[27 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-267cf80b-1e87-4cbd-b8a6-832d7ecfe41f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frota_2001</th>\n",
              "      <th>Frota_2002</th>\n",
              "      <th>Frota_2003</th>\n",
              "      <th>Frota_2004</th>\n",
              "      <th>Frota_2005</th>\n",
              "      <th>Frota_2006</th>\n",
              "      <th>Frota_2007</th>\n",
              "      <th>Frota_2008</th>\n",
              "      <th>Frota_2009</th>\n",
              "      <th>Frota_2010</th>\n",
              "      <th>...</th>\n",
              "      <th>Frota_2012</th>\n",
              "      <th>Frota_2013</th>\n",
              "      <th>Frota_2014</th>\n",
              "      <th>Frota_2015</th>\n",
              "      <th>Frota_2016</th>\n",
              "      <th>Frota_2017</th>\n",
              "      <th>Frota_2018</th>\n",
              "      <th>Frota_2019</th>\n",
              "      <th>Frota_2020</th>\n",
              "      <th>Frota_2021</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>UF</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>SP</th>\n",
              "      <td>10479722</td>\n",
              "      <td>11120530</td>\n",
              "      <td>11726049</td>\n",
              "      <td>12387248</td>\n",
              "      <td>13156526</td>\n",
              "      <td>14119987</td>\n",
              "      <td>15337079</td>\n",
              "      <td>16652255</td>\n",
              "      <td>17880303</td>\n",
              "      <td>19196525</td>\n",
              "      <td>...</td>\n",
              "      <td>21786615</td>\n",
              "      <td>22984871</td>\n",
              "      <td>24074875</td>\n",
              "      <td>24920390</td>\n",
              "      <td>25609890</td>\n",
              "      <td>26379038</td>\n",
              "      <td>27248432</td>\n",
              "      <td>28185930</td>\n",
              "      <td>28847315</td>\n",
              "      <td>29452035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MG</th>\n",
              "      <td>3095800</td>\n",
              "      <td>3304116</td>\n",
              "      <td>3532779</td>\n",
              "      <td>3762891</td>\n",
              "      <td>4037389</td>\n",
              "      <td>4382741</td>\n",
              "      <td>4831158</td>\n",
              "      <td>5363478</td>\n",
              "      <td>5879699</td>\n",
              "      <td>6460080</td>\n",
              "      <td>...</td>\n",
              "      <td>7659691</td>\n",
              "      <td>8204909</td>\n",
              "      <td>8713866</td>\n",
              "      <td>9116225</td>\n",
              "      <td>9473636</td>\n",
              "      <td>9877221</td>\n",
              "      <td>10322672</td>\n",
              "      <td>10818192</td>\n",
              "      <td>11102097</td>\n",
              "      <td>11508252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PR</th>\n",
              "      <td>2265129</td>\n",
              "      <td>2439038</td>\n",
              "      <td>2635359</td>\n",
              "      <td>2873201</td>\n",
              "      <td>3108955</td>\n",
              "      <td>3344715</td>\n",
              "      <td>3661321</td>\n",
              "      <td>4007943</td>\n",
              "      <td>4322782</td>\n",
              "      <td>4663090</td>\n",
              "      <td>...</td>\n",
              "      <td>5389621</td>\n",
              "      <td>5746038</td>\n",
              "      <td>6077970</td>\n",
              "      <td>6298966</td>\n",
              "      <td>6461473</td>\n",
              "      <td>6637325</td>\n",
              "      <td>6856556</td>\n",
              "      <td>7107117</td>\n",
              "      <td>7314208</td>\n",
              "      <td>7542118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RS</th>\n",
              "      <td>2420781</td>\n",
              "      <td>2584207</td>\n",
              "      <td>2761576</td>\n",
              "      <td>2948366</td>\n",
              "      <td>3123273</td>\n",
              "      <td>3306810</td>\n",
              "      <td>3538276</td>\n",
              "      <td>3810977</td>\n",
              "      <td>4085309</td>\n",
              "      <td>4369744</td>\n",
              "      <td>...</td>\n",
              "      <td>5020750</td>\n",
              "      <td>5359751</td>\n",
              "      <td>5664159</td>\n",
              "      <td>5883535</td>\n",
              "      <td>6058902</td>\n",
              "      <td>6242464</td>\n",
              "      <td>6448570</td>\n",
              "      <td>6656528</td>\n",
              "      <td>6822109</td>\n",
              "      <td>6987084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RJ</th>\n",
              "      <td>2420433</td>\n",
              "      <td>2589084</td>\n",
              "      <td>2723852</td>\n",
              "      <td>2857823</td>\n",
              "      <td>3003923</td>\n",
              "      <td>3170707</td>\n",
              "      <td>3404576</td>\n",
              "      <td>3669995</td>\n",
              "      <td>3939390</td>\n",
              "      <td>4254169</td>\n",
              "      <td>...</td>\n",
              "      <td>4936331</td>\n",
              "      <td>5270462</td>\n",
              "      <td>5594401</td>\n",
              "      <td>5839571</td>\n",
              "      <td>6013457</td>\n",
              "      <td>6165834</td>\n",
              "      <td>6342302</td>\n",
              "      <td>6557189</td>\n",
              "      <td>6688780</td>\n",
              "      <td>6886291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SC</th>\n",
              "      <td>1425531</td>\n",
              "      <td>1557072</td>\n",
              "      <td>1697672</td>\n",
              "      <td>1855680</td>\n",
              "      <td>2030201</td>\n",
              "      <td>2215862</td>\n",
              "      <td>2436675</td>\n",
              "      <td>2656860</td>\n",
              "      <td>2886284</td>\n",
              "      <td>3133685</td>\n",
              "      <td>...</td>\n",
              "      <td>3622811</td>\n",
              "      <td>3860577</td>\n",
              "      <td>4083148</td>\n",
              "      <td>4246772</td>\n",
              "      <td>4383715</td>\n",
              "      <td>4543946</td>\n",
              "      <td>4730433</td>\n",
              "      <td>4936317</td>\n",
              "      <td>5108671</td>\n",
              "      <td>5278406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>BA</th>\n",
              "      <td>795140</td>\n",
              "      <td>883017</td>\n",
              "      <td>973975</td>\n",
              "      <td>1064157</td>\n",
              "      <td>1173092</td>\n",
              "      <td>1297258</td>\n",
              "      <td>1453960</td>\n",
              "      <td>1648855</td>\n",
              "      <td>1873149</td>\n",
              "      <td>2124487</td>\n",
              "      <td>...</td>\n",
              "      <td>2652076</td>\n",
              "      <td>2912176</td>\n",
              "      <td>3163222</td>\n",
              "      <td>3365071</td>\n",
              "      <td>3516255</td>\n",
              "      <td>3662126</td>\n",
              "      <td>3823653</td>\n",
              "      <td>4001194</td>\n",
              "      <td>4159317</td>\n",
              "      <td>4328600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>GO</th>\n",
              "      <td>926122</td>\n",
              "      <td>1011642</td>\n",
              "      <td>1098583</td>\n",
              "      <td>1193607</td>\n",
              "      <td>1297352</td>\n",
              "      <td>1417705</td>\n",
              "      <td>1592651</td>\n",
              "      <td>1797672</td>\n",
              "      <td>1987496</td>\n",
              "      <td>2195945</td>\n",
              "      <td>...</td>\n",
              "      <td>2635671</td>\n",
              "      <td>2842455</td>\n",
              "      <td>3028799</td>\n",
              "      <td>3165076</td>\n",
              "      <td>3261283</td>\n",
              "      <td>3362996</td>\n",
              "      <td>3483471</td>\n",
              "      <td>3609048</td>\n",
              "      <td>3731384</td>\n",
              "      <td>3868162</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>CE</th>\n",
              "      <td>647275</td>\n",
              "      <td>711101</td>\n",
              "      <td>772022</td>\n",
              "      <td>831054</td>\n",
              "      <td>898707</td>\n",
              "      <td>988200</td>\n",
              "      <td>1108077</td>\n",
              "      <td>1240437</td>\n",
              "      <td>1404090</td>\n",
              "      <td>1613903</td>\n",
              "      <td>...</td>\n",
              "      <td>2048665</td>\n",
              "      <td>2256965</td>\n",
              "      <td>2458806</td>\n",
              "      <td>2632028</td>\n",
              "      <td>2752124</td>\n",
              "      <td>2858928</td>\n",
              "      <td>2975118</td>\n",
              "      <td>3102748</td>\n",
              "      <td>3209041</td>\n",
              "      <td>3319156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PE</th>\n",
              "      <td>724389</td>\n",
              "      <td>788089</td>\n",
              "      <td>842213</td>\n",
              "      <td>900404</td>\n",
              "      <td>965805</td>\n",
              "      <td>1048077</td>\n",
              "      <td>1158546</td>\n",
              "      <td>1292762</td>\n",
              "      <td>1449215</td>\n",
              "      <td>1643907</td>\n",
              "      <td>...</td>\n",
              "      <td>2045263</td>\n",
              "      <td>2226463</td>\n",
              "      <td>2387299</td>\n",
              "      <td>2505332</td>\n",
              "      <td>2585182</td>\n",
              "      <td>2669558</td>\n",
              "      <td>2766024</td>\n",
              "      <td>2876606</td>\n",
              "      <td>2963364</td>\n",
              "      <td>3065027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PA</th>\n",
              "      <td>279713</td>\n",
              "      <td>313054</td>\n",
              "      <td>353391</td>\n",
              "      <td>397715</td>\n",
              "      <td>448652</td>\n",
              "      <td>501158</td>\n",
              "      <td>575899</td>\n",
              "      <td>671371</td>\n",
              "      <td>776604</td>\n",
              "      <td>890856</td>\n",
              "      <td>...</td>\n",
              "      <td>1167985</td>\n",
              "      <td>1320954</td>\n",
              "      <td>1468345</td>\n",
              "      <td>1596959</td>\n",
              "      <td>1694592</td>\n",
              "      <td>1779613</td>\n",
              "      <td>1869823</td>\n",
              "      <td>1968561</td>\n",
              "      <td>2070053</td>\n",
              "      <td>2182858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MT</th>\n",
              "      <td>364758</td>\n",
              "      <td>413019</td>\n",
              "      <td>465387</td>\n",
              "      <td>527584</td>\n",
              "      <td>586913</td>\n",
              "      <td>639597</td>\n",
              "      <td>719262</td>\n",
              "      <td>832873</td>\n",
              "      <td>938475</td>\n",
              "      <td>1048751</td>\n",
              "      <td>...</td>\n",
              "      <td>1277180</td>\n",
              "      <td>1393932</td>\n",
              "      <td>1509614</td>\n",
              "      <td>1600270</td>\n",
              "      <td>1678056</td>\n",
              "      <td>1753049</td>\n",
              "      <td>1852456</td>\n",
              "      <td>1960549</td>\n",
              "      <td>2053994</td>\n",
              "      <td>2152577</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ES</th>\n",
              "      <td>483331</td>\n",
              "      <td>524359</td>\n",
              "      <td>564959</td>\n",
              "      <td>613695</td>\n",
              "      <td>668692</td>\n",
              "      <td>737878</td>\n",
              "      <td>834105</td>\n",
              "      <td>944809</td>\n",
              "      <td>1050233</td>\n",
              "      <td>1141578</td>\n",
              "      <td>...</td>\n",
              "      <td>1342793</td>\n",
              "      <td>1437215</td>\n",
              "      <td>1528510</td>\n",
              "      <td>1595033</td>\n",
              "      <td>1644748</td>\n",
              "      <td>1697352</td>\n",
              "      <td>1759414</td>\n",
              "      <td>1826559</td>\n",
              "      <td>1887361</td>\n",
              "      <td>1960489</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DF</th>\n",
              "      <td>610273</td>\n",
              "      <td>651501</td>\n",
              "      <td>693136</td>\n",
              "      <td>736805</td>\n",
              "      <td>782578</td>\n",
              "      <td>845577</td>\n",
              "      <td>926080</td>\n",
              "      <td>1006954</td>\n",
              "      <td>1096593</td>\n",
              "      <td>1190062</td>\n",
              "      <td>...</td>\n",
              "      <td>1361430</td>\n",
              "      <td>1447068</td>\n",
              "      <td>1518361</td>\n",
              "      <td>1578644</td>\n",
              "      <td>1626709</td>\n",
              "      <td>1677003</td>\n",
              "      <td>1735359</td>\n",
              "      <td>1804081</td>\n",
              "      <td>1851271</td>\n",
              "      <td>1894859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MA</th>\n",
              "      <td>207170</td>\n",
              "      <td>231810</td>\n",
              "      <td>261459</td>\n",
              "      <td>294018</td>\n",
              "      <td>336706</td>\n",
              "      <td>384945</td>\n",
              "      <td>451681</td>\n",
              "      <td>529524</td>\n",
              "      <td>627324</td>\n",
              "      <td>752323</td>\n",
              "      <td>...</td>\n",
              "      <td>1020563</td>\n",
              "      <td>1149680</td>\n",
              "      <td>1272622</td>\n",
              "      <td>1381272</td>\n",
              "      <td>1458747</td>\n",
              "      <td>1532211</td>\n",
              "      <td>1605403</td>\n",
              "      <td>1680568</td>\n",
              "      <td>1754277</td>\n",
              "      <td>1832383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MS</th>\n",
              "      <td>386848</td>\n",
              "      <td>425638</td>\n",
              "      <td>462951</td>\n",
              "      <td>505641</td>\n",
              "      <td>550667</td>\n",
              "      <td>596871</td>\n",
              "      <td>658558</td>\n",
              "      <td>728997</td>\n",
              "      <td>800146</td>\n",
              "      <td>882658</td>\n",
              "      <td>...</td>\n",
              "      <td>1052495</td>\n",
              "      <td>1138186</td>\n",
              "      <td>1215773</td>\n",
              "      <td>1276071</td>\n",
              "      <td>1322279</td>\n",
              "      <td>1373073</td>\n",
              "      <td>1429496</td>\n",
              "      <td>1488085</td>\n",
              "      <td>1536874</td>\n",
              "      <td>1583331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PB</th>\n",
              "      <td>251149</td>\n",
              "      <td>275795</td>\n",
              "      <td>301453</td>\n",
              "      <td>325359</td>\n",
              "      <td>354174</td>\n",
              "      <td>393191</td>\n",
              "      <td>447845</td>\n",
              "      <td>511993</td>\n",
              "      <td>579852</td>\n",
              "      <td>662730</td>\n",
              "      <td>...</td>\n",
              "      <td>835750</td>\n",
              "      <td>911963</td>\n",
              "      <td>985639</td>\n",
              "      <td>1052095</td>\n",
              "      <td>1104083</td>\n",
              "      <td>1153643</td>\n",
              "      <td>1207180</td>\n",
              "      <td>1263944</td>\n",
              "      <td>1316572</td>\n",
              "      <td>1373274</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RN</th>\n",
              "      <td>255075</td>\n",
              "      <td>288605</td>\n",
              "      <td>314915</td>\n",
              "      <td>345157</td>\n",
              "      <td>380123</td>\n",
              "      <td>425175</td>\n",
              "      <td>487482</td>\n",
              "      <td>551541</td>\n",
              "      <td>614950</td>\n",
              "      <td>690175</td>\n",
              "      <td>...</td>\n",
              "      <td>838659</td>\n",
              "      <td>912976</td>\n",
              "      <td>984481</td>\n",
              "      <td>1045065</td>\n",
              "      <td>1093746</td>\n",
              "      <td>1142962</td>\n",
              "      <td>1191130</td>\n",
              "      <td>1241174</td>\n",
              "      <td>1281267</td>\n",
              "      <td>1329223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PI</th>\n",
              "      <td>170444</td>\n",
              "      <td>194811</td>\n",
              "      <td>220234</td>\n",
              "      <td>246278</td>\n",
              "      <td>275378</td>\n",
              "      <td>312579</td>\n",
              "      <td>355889</td>\n",
              "      <td>412045</td>\n",
              "      <td>475404</td>\n",
              "      <td>553349</td>\n",
              "      <td>...</td>\n",
              "      <td>722195</td>\n",
              "      <td>810963</td>\n",
              "      <td>898204</td>\n",
              "      <td>971345</td>\n",
              "      <td>1028808</td>\n",
              "      <td>1081116</td>\n",
              "      <td>1132902</td>\n",
              "      <td>1183315</td>\n",
              "      <td>1221031</td>\n",
              "      <td>1265094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RO</th>\n",
              "      <td>170331</td>\n",
              "      <td>190842</td>\n",
              "      <td>212130</td>\n",
              "      <td>239601</td>\n",
              "      <td>267639</td>\n",
              "      <td>298040</td>\n",
              "      <td>337834</td>\n",
              "      <td>391888</td>\n",
              "      <td>451941</td>\n",
              "      <td>516948</td>\n",
              "      <td>...</td>\n",
              "      <td>647049</td>\n",
              "      <td>701403</td>\n",
              "      <td>755698</td>\n",
              "      <td>800807</td>\n",
              "      <td>835837</td>\n",
              "      <td>868641</td>\n",
              "      <td>909027</td>\n",
              "      <td>951167</td>\n",
              "      <td>987460</td>\n",
              "      <td>1025606</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AM</th>\n",
              "      <td>183930</td>\n",
              "      <td>203137</td>\n",
              "      <td>223124</td>\n",
              "      <td>248669</td>\n",
              "      <td>280997</td>\n",
              "      <td>312597</td>\n",
              "      <td>348992</td>\n",
              "      <td>391739</td>\n",
              "      <td>436955</td>\n",
              "      <td>490989</td>\n",
              "      <td>...</td>\n",
              "      <td>596725</td>\n",
              "      <td>651120</td>\n",
              "      <td>703733</td>\n",
              "      <td>741684</td>\n",
              "      <td>765409</td>\n",
              "      <td>792599</td>\n",
              "      <td>827751</td>\n",
              "      <td>871225</td>\n",
              "      <td>910285</td>\n",
              "      <td>951120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AL</th>\n",
              "      <td>164779</td>\n",
              "      <td>179502</td>\n",
              "      <td>196505</td>\n",
              "      <td>212548</td>\n",
              "      <td>231011</td>\n",
              "      <td>252801</td>\n",
              "      <td>280680</td>\n",
              "      <td>313604</td>\n",
              "      <td>353304</td>\n",
              "      <td>401113</td>\n",
              "      <td>...</td>\n",
              "      <td>510163</td>\n",
              "      <td>564781</td>\n",
              "      <td>618177</td>\n",
              "      <td>661096</td>\n",
              "      <td>694703</td>\n",
              "      <td>730176</td>\n",
              "      <td>767520</td>\n",
              "      <td>812919</td>\n",
              "      <td>849278</td>\n",
              "      <td>894965</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SE</th>\n",
              "      <td>160162</td>\n",
              "      <td>176127</td>\n",
              "      <td>192342</td>\n",
              "      <td>207787</td>\n",
              "      <td>224057</td>\n",
              "      <td>243795</td>\n",
              "      <td>270877</td>\n",
              "      <td>302716</td>\n",
              "      <td>345441</td>\n",
              "      <td>393289</td>\n",
              "      <td>...</td>\n",
              "      <td>489313</td>\n",
              "      <td>530907</td>\n",
              "      <td>573974</td>\n",
              "      <td>609505</td>\n",
              "      <td>635716</td>\n",
              "      <td>662561</td>\n",
              "      <td>690040</td>\n",
              "      <td>720170</td>\n",
              "      <td>746440</td>\n",
              "      <td>777144</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>TO</th>\n",
              "      <td>92676</td>\n",
              "      <td>112953</td>\n",
              "      <td>133909</td>\n",
              "      <td>156266</td>\n",
              "      <td>184066</td>\n",
              "      <td>225052</td>\n",
              "      <td>253822</td>\n",
              "      <td>284106</td>\n",
              "      <td>315328</td>\n",
              "      <td>354658</td>\n",
              "      <td>...</td>\n",
              "      <td>434027</td>\n",
              "      <td>474648</td>\n",
              "      <td>514457</td>\n",
              "      <td>550332</td>\n",
              "      <td>573433</td>\n",
              "      <td>595586</td>\n",
              "      <td>620909</td>\n",
              "      <td>648467</td>\n",
              "      <td>675708</td>\n",
              "      <td>706998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AC</th>\n",
              "      <td>43151</td>\n",
              "      <td>48474</td>\n",
              "      <td>54356</td>\n",
              "      <td>60739</td>\n",
              "      <td>67967</td>\n",
              "      <td>77072</td>\n",
              "      <td>89145</td>\n",
              "      <td>104315</td>\n",
              "      <td>122393</td>\n",
              "      <td>142994</td>\n",
              "      <td>...</td>\n",
              "      <td>178288</td>\n",
              "      <td>194603</td>\n",
              "      <td>211329</td>\n",
              "      <td>226569</td>\n",
              "      <td>237604</td>\n",
              "      <td>249261</td>\n",
              "      <td>262595</td>\n",
              "      <td>276579</td>\n",
              "      <td>289009</td>\n",
              "      <td>301858</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RR</th>\n",
              "      <td>38975</td>\n",
              "      <td>44850</td>\n",
              "      <td>50553</td>\n",
              "      <td>55557</td>\n",
              "      <td>60583</td>\n",
              "      <td>67949</td>\n",
              "      <td>80060</td>\n",
              "      <td>92200</td>\n",
              "      <td>104589</td>\n",
              "      <td>119429</td>\n",
              "      <td>...</td>\n",
              "      <td>144839</td>\n",
              "      <td>157285</td>\n",
              "      <td>170007</td>\n",
              "      <td>181398</td>\n",
              "      <td>190774</td>\n",
              "      <td>199510</td>\n",
              "      <td>207729</td>\n",
              "      <td>216525</td>\n",
              "      <td>225580</td>\n",
              "      <td>235813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>AP</th>\n",
              "      <td>30558</td>\n",
              "      <td>35419</td>\n",
              "      <td>40026</td>\n",
              "      <td>45570</td>\n",
              "      <td>52973</td>\n",
              "      <td>61704</td>\n",
              "      <td>71073</td>\n",
              "      <td>82001</td>\n",
              "      <td>94532</td>\n",
              "      <td>109772</td>\n",
              "      <td>...</td>\n",
              "      <td>134262</td>\n",
              "      <td>144815</td>\n",
              "      <td>156261</td>\n",
              "      <td>165206</td>\n",
              "      <td>170255</td>\n",
              "      <td>176746</td>\n",
              "      <td>185038</td>\n",
              "      <td>195012</td>\n",
              "      <td>204414</td>\n",
              "      <td>212706</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>27 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-267cf80b-1e87-4cbd-b8a6-832d7ecfe41f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-267cf80b-1e87-4cbd-b8a6-832d7ecfe41f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-267cf80b-1e87-4cbd-b8a6-832d7ecfe41f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Para fazer o one-hot-encoding, selecionamos os 6 estados com maior frota\n",
        "# atualmente e colocamos os outros como \"OUTROS\". Porque senão é um número\n",
        "# muito grande para ohe\n",
        "\n",
        "uf = ['SP', 'MG', 'PR', 'RS', 'RJ', 'SC']\n",
        "l['uf_cat'] = np.where(l['UF'].isin(uf), l['UF'], 'OUTRO')\n",
        "\n",
        "l.groupby(\"uf_cat\").sum().sort_values(\"Frota_2021\", ascending=False)\n",
        "# pd.get_dummies(l['uf_cat'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "iNmn9QegXrhc",
        "outputId": "42231f98-00dc-4c7a-eb5d-c0e63e4c0991"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        Frota_2001  Frota_2002  Frota_2003  Frota_2004  Frota_2005  \\\n",
              "uf_cat                                                               \n",
              "OUTRO      6986249     7703745     8427623     9208211    10088132   \n",
              "SP        10479722    11120530    11726049    12387248    13156526   \n",
              "MG         3095800     3304116     3532779     3762891     4037389   \n",
              "PR         2265129     2439038     2635359     2873201     3108955   \n",
              "RS         2420781     2584207     2761576     2948366     3123273   \n",
              "RJ         2420433     2589084     2723852     2857823     3003923   \n",
              "SC         1425531     1557072     1697672     1855680     2030201   \n",
              "\n",
              "        Frota_2006  Frota_2007  Frota_2008  Frota_2009  Frota_2010  ...  \\\n",
              "uf_cat                                                              ...   \n",
              "OUTRO     11127221    12502518    14132402    15898014    17919916  ...   \n",
              "SP        14119987    15337079    16652255    17880303    19196525  ...   \n",
              "MG         4382741     4831158     5363478     5879699     6460080  ...   \n",
              "PR         3344715     3661321     4007943     4322782     4663090  ...   \n",
              "RS         3306810     3538276     3810977     4085309     4369744  ...   \n",
              "RJ         3170707     3404576     3669995     3939390     4254169  ...   \n",
              "SC         2215862     2436675     2656860     2886284     3133685  ...   \n",
              "\n",
              "        Frota_2012  Frota_2013  Frota_2014  Frota_2015  Frota_2016  \\\n",
              "uf_cat                                                               \n",
              "OUTRO     22135391    24180558    26123311    27700858    28870343   \n",
              "SP        21786615    22984871    24074875    24920390    25609890   \n",
              "MG         7659691     8204909     8713866     9116225     9473636   \n",
              "PR         5389621     5746038     6077970     6298966     6461473   \n",
              "RS         5020750     5359751     5664159     5883535     6058902   \n",
              "RJ         4936331     5270462     5594401     5839571     6013457   \n",
              "SC         3622811     3860577     4083148     4246772     4383715   \n",
              "\n",
              "        Frota_2017  Frota_2018  Frota_2019  Frota_2020  Frota_2021  \n",
              "uf_cat                                                              \n",
              "OUTRO     30018710    31302038    32698496    33923980    35261243  \n",
              "SP        26379038    27248432    28185930    28847315    29452035  \n",
              "MG         9877221    10322672    10818192    11102097    11508252  \n",
              "PR         6637325     6856556     7107117     7314208     7542118  \n",
              "RS         6242464     6448570     6656528     6822109     6987084  \n",
              "RJ         6165834     6342302     6557189     6688780     6886291  \n",
              "SC         4543946     4730433     4936317     5108671     5278406  \n",
              "\n",
              "[7 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a79d12b8-00d8-4a95-96fc-d650b8b82a1e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frota_2001</th>\n",
              "      <th>Frota_2002</th>\n",
              "      <th>Frota_2003</th>\n",
              "      <th>Frota_2004</th>\n",
              "      <th>Frota_2005</th>\n",
              "      <th>Frota_2006</th>\n",
              "      <th>Frota_2007</th>\n",
              "      <th>Frota_2008</th>\n",
              "      <th>Frota_2009</th>\n",
              "      <th>Frota_2010</th>\n",
              "      <th>...</th>\n",
              "      <th>Frota_2012</th>\n",
              "      <th>Frota_2013</th>\n",
              "      <th>Frota_2014</th>\n",
              "      <th>Frota_2015</th>\n",
              "      <th>Frota_2016</th>\n",
              "      <th>Frota_2017</th>\n",
              "      <th>Frota_2018</th>\n",
              "      <th>Frota_2019</th>\n",
              "      <th>Frota_2020</th>\n",
              "      <th>Frota_2021</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>uf_cat</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>OUTRO</th>\n",
              "      <td>6986249</td>\n",
              "      <td>7703745</td>\n",
              "      <td>8427623</td>\n",
              "      <td>9208211</td>\n",
              "      <td>10088132</td>\n",
              "      <td>11127221</td>\n",
              "      <td>12502518</td>\n",
              "      <td>14132402</td>\n",
              "      <td>15898014</td>\n",
              "      <td>17919916</td>\n",
              "      <td>...</td>\n",
              "      <td>22135391</td>\n",
              "      <td>24180558</td>\n",
              "      <td>26123311</td>\n",
              "      <td>27700858</td>\n",
              "      <td>28870343</td>\n",
              "      <td>30018710</td>\n",
              "      <td>31302038</td>\n",
              "      <td>32698496</td>\n",
              "      <td>33923980</td>\n",
              "      <td>35261243</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SP</th>\n",
              "      <td>10479722</td>\n",
              "      <td>11120530</td>\n",
              "      <td>11726049</td>\n",
              "      <td>12387248</td>\n",
              "      <td>13156526</td>\n",
              "      <td>14119987</td>\n",
              "      <td>15337079</td>\n",
              "      <td>16652255</td>\n",
              "      <td>17880303</td>\n",
              "      <td>19196525</td>\n",
              "      <td>...</td>\n",
              "      <td>21786615</td>\n",
              "      <td>22984871</td>\n",
              "      <td>24074875</td>\n",
              "      <td>24920390</td>\n",
              "      <td>25609890</td>\n",
              "      <td>26379038</td>\n",
              "      <td>27248432</td>\n",
              "      <td>28185930</td>\n",
              "      <td>28847315</td>\n",
              "      <td>29452035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MG</th>\n",
              "      <td>3095800</td>\n",
              "      <td>3304116</td>\n",
              "      <td>3532779</td>\n",
              "      <td>3762891</td>\n",
              "      <td>4037389</td>\n",
              "      <td>4382741</td>\n",
              "      <td>4831158</td>\n",
              "      <td>5363478</td>\n",
              "      <td>5879699</td>\n",
              "      <td>6460080</td>\n",
              "      <td>...</td>\n",
              "      <td>7659691</td>\n",
              "      <td>8204909</td>\n",
              "      <td>8713866</td>\n",
              "      <td>9116225</td>\n",
              "      <td>9473636</td>\n",
              "      <td>9877221</td>\n",
              "      <td>10322672</td>\n",
              "      <td>10818192</td>\n",
              "      <td>11102097</td>\n",
              "      <td>11508252</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>PR</th>\n",
              "      <td>2265129</td>\n",
              "      <td>2439038</td>\n",
              "      <td>2635359</td>\n",
              "      <td>2873201</td>\n",
              "      <td>3108955</td>\n",
              "      <td>3344715</td>\n",
              "      <td>3661321</td>\n",
              "      <td>4007943</td>\n",
              "      <td>4322782</td>\n",
              "      <td>4663090</td>\n",
              "      <td>...</td>\n",
              "      <td>5389621</td>\n",
              "      <td>5746038</td>\n",
              "      <td>6077970</td>\n",
              "      <td>6298966</td>\n",
              "      <td>6461473</td>\n",
              "      <td>6637325</td>\n",
              "      <td>6856556</td>\n",
              "      <td>7107117</td>\n",
              "      <td>7314208</td>\n",
              "      <td>7542118</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RS</th>\n",
              "      <td>2420781</td>\n",
              "      <td>2584207</td>\n",
              "      <td>2761576</td>\n",
              "      <td>2948366</td>\n",
              "      <td>3123273</td>\n",
              "      <td>3306810</td>\n",
              "      <td>3538276</td>\n",
              "      <td>3810977</td>\n",
              "      <td>4085309</td>\n",
              "      <td>4369744</td>\n",
              "      <td>...</td>\n",
              "      <td>5020750</td>\n",
              "      <td>5359751</td>\n",
              "      <td>5664159</td>\n",
              "      <td>5883535</td>\n",
              "      <td>6058902</td>\n",
              "      <td>6242464</td>\n",
              "      <td>6448570</td>\n",
              "      <td>6656528</td>\n",
              "      <td>6822109</td>\n",
              "      <td>6987084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RJ</th>\n",
              "      <td>2420433</td>\n",
              "      <td>2589084</td>\n",
              "      <td>2723852</td>\n",
              "      <td>2857823</td>\n",
              "      <td>3003923</td>\n",
              "      <td>3170707</td>\n",
              "      <td>3404576</td>\n",
              "      <td>3669995</td>\n",
              "      <td>3939390</td>\n",
              "      <td>4254169</td>\n",
              "      <td>...</td>\n",
              "      <td>4936331</td>\n",
              "      <td>5270462</td>\n",
              "      <td>5594401</td>\n",
              "      <td>5839571</td>\n",
              "      <td>6013457</td>\n",
              "      <td>6165834</td>\n",
              "      <td>6342302</td>\n",
              "      <td>6557189</td>\n",
              "      <td>6688780</td>\n",
              "      <td>6886291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SC</th>\n",
              "      <td>1425531</td>\n",
              "      <td>1557072</td>\n",
              "      <td>1697672</td>\n",
              "      <td>1855680</td>\n",
              "      <td>2030201</td>\n",
              "      <td>2215862</td>\n",
              "      <td>2436675</td>\n",
              "      <td>2656860</td>\n",
              "      <td>2886284</td>\n",
              "      <td>3133685</td>\n",
              "      <td>...</td>\n",
              "      <td>3622811</td>\n",
              "      <td>3860577</td>\n",
              "      <td>4083148</td>\n",
              "      <td>4246772</td>\n",
              "      <td>4383715</td>\n",
              "      <td>4543946</td>\n",
              "      <td>4730433</td>\n",
              "      <td>4936317</td>\n",
              "      <td>5108671</td>\n",
              "      <td>5278406</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a79d12b8-00d8-4a95-96fc-d650b8b82a1e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a79d12b8-00d8-4a95-96fc-d650b8b82a1e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a79d12b8-00d8-4a95-96fc-d650b8b82a1e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l['CAPITAL'] = l['CAPITAL'].astype(\"category\")"
      ],
      "metadata": {
        "id": "7mwO9p7Djcl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l.dtypes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpOGf3_Ha1xi",
        "outputId": "a488244c-4e1f-47a5-f482-32c5ab6b84cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UF              object\n",
              "CAPITAL       category\n",
              "Frota_2001       int64\n",
              "Frota_2002       int64\n",
              "Frota_2003       int64\n",
              "Frota_2004       int64\n",
              "Frota_2005       int64\n",
              "Frota_2006       int64\n",
              "Frota_2007       int64\n",
              "Frota_2008       int64\n",
              "Frota_2009       int64\n",
              "Frota_2010       int64\n",
              "Frota_2011       int64\n",
              "Frota_2012       int64\n",
              "Frota_2013       int64\n",
              "Frota_2014       int64\n",
              "Frota_2015       int64\n",
              "Frota_2016       int64\n",
              "Frota_2017       int64\n",
              "Frota_2018       int64\n",
              "Frota_2019       int64\n",
              "Frota_2020       int64\n",
              "Frota_2021       int64\n",
              "uf_cat          object\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ohe = OneHotEncoder()\n",
        "arr = ohe.fit_transform(l[[\"uf_cat\"]]).toarray()\n",
        "arr"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VOVLgwSuc9g3",
        "outputId": "d7bfd8ad-841d-42e2-a715-7749a1a05be7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ohe.categories_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYVpDefmf0zR",
        "outputId": "d0524254-5638-4475-e05b-c8130f58c104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array(['MG', 'OUTRO', 'PR', 'RJ', 'RS', 'SC', 'SP'], dtype=object)]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l = l.drop(\"UF\", axis=1).drop(\"uf_cat\", axis=1).drop(\"CAPITAL\", axis=1)"
      ],
      "metadata": {
        "id": "vn29B5OheL4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "7nYiF3IkeRS4",
        "outputId": "18d74b12-431c-4b5d-f22b-1cbfa3aa1f59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Frota_2001  Frota_2002  Frota_2003  Frota_2004  Frota_2005  Frota_2006  \\\n",
              "0         148         254         380         605         847        1061   \n",
              "1          44          65          87         115         159         236   \n",
              "2         683         874        1064        1228        1468        1749   \n",
              "3          34          55         107         157         219         274   \n",
              "4          36          65          98         129         171         239   \n",
              "\n",
              "   Frota_2007  Frota_2008  Frota_2009  Frota_2010  ...  Frota_2012  \\\n",
              "0        1288        1627        1974        2368  ...        3187   \n",
              "1         294         366         439         525  ...         795   \n",
              "2        2117        2565        3095        3669  ...        4737   \n",
              "3         336         441         562         711  ...         993   \n",
              "4         348         449         567         741  ...        1008   \n",
              "\n",
              "   Frota_2013  Frota_2014  Frota_2015  Frota_2016  Frota_2017  Frota_2018  \\\n",
              "0        3543        3971        4364        4648        4856        5163   \n",
              "1         940        1119        1271        1363        1485        1615   \n",
              "2        5329        6017        6697        7235        7638        8174   \n",
              "3        1146        1309        1434        1570        1652        1780   \n",
              "4        1154        1287        1468        1635        1748        1890   \n",
              "\n",
              "   Frota_2019  Frota_2020  Frota_2021  \n",
              "0        5541        6045        6465  \n",
              "1        1700        1879        2019  \n",
              "2        8840        9450       10280  \n",
              "3        1860        2022        2172  \n",
              "4        2003        2167        2334  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-07e71fea-c746-4d8d-a228-9469475df721\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frota_2001</th>\n",
              "      <th>Frota_2002</th>\n",
              "      <th>Frota_2003</th>\n",
              "      <th>Frota_2004</th>\n",
              "      <th>Frota_2005</th>\n",
              "      <th>Frota_2006</th>\n",
              "      <th>Frota_2007</th>\n",
              "      <th>Frota_2008</th>\n",
              "      <th>Frota_2009</th>\n",
              "      <th>Frota_2010</th>\n",
              "      <th>...</th>\n",
              "      <th>Frota_2012</th>\n",
              "      <th>Frota_2013</th>\n",
              "      <th>Frota_2014</th>\n",
              "      <th>Frota_2015</th>\n",
              "      <th>Frota_2016</th>\n",
              "      <th>Frota_2017</th>\n",
              "      <th>Frota_2018</th>\n",
              "      <th>Frota_2019</th>\n",
              "      <th>Frota_2020</th>\n",
              "      <th>Frota_2021</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>148</td>\n",
              "      <td>254</td>\n",
              "      <td>380</td>\n",
              "      <td>605</td>\n",
              "      <td>847</td>\n",
              "      <td>1061</td>\n",
              "      <td>1288</td>\n",
              "      <td>1627</td>\n",
              "      <td>1974</td>\n",
              "      <td>2368</td>\n",
              "      <td>...</td>\n",
              "      <td>3187</td>\n",
              "      <td>3543</td>\n",
              "      <td>3971</td>\n",
              "      <td>4364</td>\n",
              "      <td>4648</td>\n",
              "      <td>4856</td>\n",
              "      <td>5163</td>\n",
              "      <td>5541</td>\n",
              "      <td>6045</td>\n",
              "      <td>6465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44</td>\n",
              "      <td>65</td>\n",
              "      <td>87</td>\n",
              "      <td>115</td>\n",
              "      <td>159</td>\n",
              "      <td>236</td>\n",
              "      <td>294</td>\n",
              "      <td>366</td>\n",
              "      <td>439</td>\n",
              "      <td>525</td>\n",
              "      <td>...</td>\n",
              "      <td>795</td>\n",
              "      <td>940</td>\n",
              "      <td>1119</td>\n",
              "      <td>1271</td>\n",
              "      <td>1363</td>\n",
              "      <td>1485</td>\n",
              "      <td>1615</td>\n",
              "      <td>1700</td>\n",
              "      <td>1879</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>683</td>\n",
              "      <td>874</td>\n",
              "      <td>1064</td>\n",
              "      <td>1228</td>\n",
              "      <td>1468</td>\n",
              "      <td>1749</td>\n",
              "      <td>2117</td>\n",
              "      <td>2565</td>\n",
              "      <td>3095</td>\n",
              "      <td>3669</td>\n",
              "      <td>...</td>\n",
              "      <td>4737</td>\n",
              "      <td>5329</td>\n",
              "      <td>6017</td>\n",
              "      <td>6697</td>\n",
              "      <td>7235</td>\n",
              "      <td>7638</td>\n",
              "      <td>8174</td>\n",
              "      <td>8840</td>\n",
              "      <td>9450</td>\n",
              "      <td>10280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>34</td>\n",
              "      <td>55</td>\n",
              "      <td>107</td>\n",
              "      <td>157</td>\n",
              "      <td>219</td>\n",
              "      <td>274</td>\n",
              "      <td>336</td>\n",
              "      <td>441</td>\n",
              "      <td>562</td>\n",
              "      <td>711</td>\n",
              "      <td>...</td>\n",
              "      <td>993</td>\n",
              "      <td>1146</td>\n",
              "      <td>1309</td>\n",
              "      <td>1434</td>\n",
              "      <td>1570</td>\n",
              "      <td>1652</td>\n",
              "      <td>1780</td>\n",
              "      <td>1860</td>\n",
              "      <td>2022</td>\n",
              "      <td>2172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>36</td>\n",
              "      <td>65</td>\n",
              "      <td>98</td>\n",
              "      <td>129</td>\n",
              "      <td>171</td>\n",
              "      <td>239</td>\n",
              "      <td>348</td>\n",
              "      <td>449</td>\n",
              "      <td>567</td>\n",
              "      <td>741</td>\n",
              "      <td>...</td>\n",
              "      <td>1008</td>\n",
              "      <td>1154</td>\n",
              "      <td>1287</td>\n",
              "      <td>1468</td>\n",
              "      <td>1635</td>\n",
              "      <td>1748</td>\n",
              "      <td>1890</td>\n",
              "      <td>2003</td>\n",
              "      <td>2167</td>\n",
              "      <td>2334</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07e71fea-c746-4d8d-a228-9469475df721')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-07e71fea-c746-4d8d-a228-9469475df721 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-07e71fea-c746-4d8d-a228-9469475df721');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Regressão\n",
        "\n",
        "Escolhemos trabalhar com a regressão para predizer a Frota_2021 a partir dos dados dos anos anteriores"
      ],
      "metadata": {
        "id": "rOPNM-Yol4du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "szxevjVPl4NE",
        "outputId": "2822b8e1-a1ad-4c7d-d153-47d50dc35bd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Frota_2001  Frota_2002  Frota_2003  Frota_2004  Frota_2005  Frota_2006  \\\n",
              "0         148         254         380         605         847        1061   \n",
              "1          44          65          87         115         159         236   \n",
              "2         683         874        1064        1228        1468        1749   \n",
              "3          34          55         107         157         219         274   \n",
              "4          36          65          98         129         171         239   \n",
              "\n",
              "   Frota_2007  Frota_2008  Frota_2009  Frota_2010  ...  Frota_2012  \\\n",
              "0        1288        1627        1974        2368  ...        3187   \n",
              "1         294         366         439         525  ...         795   \n",
              "2        2117        2565        3095        3669  ...        4737   \n",
              "3         336         441         562         711  ...         993   \n",
              "4         348         449         567         741  ...        1008   \n",
              "\n",
              "   Frota_2013  Frota_2014  Frota_2015  Frota_2016  Frota_2017  Frota_2018  \\\n",
              "0        3543        3971        4364        4648        4856        5163   \n",
              "1         940        1119        1271        1363        1485        1615   \n",
              "2        5329        6017        6697        7235        7638        8174   \n",
              "3        1146        1309        1434        1570        1652        1780   \n",
              "4        1154        1287        1468        1635        1748        1890   \n",
              "\n",
              "   Frota_2019  Frota_2020  Frota_2021  \n",
              "0        5541        6045        6465  \n",
              "1        1700        1879        2019  \n",
              "2        8840        9450       10280  \n",
              "3        1860        2022        2172  \n",
              "4        2003        2167        2334  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a1d490ec-37d4-499e-b1a3-ad7ebcecc035\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Frota_2001</th>\n",
              "      <th>Frota_2002</th>\n",
              "      <th>Frota_2003</th>\n",
              "      <th>Frota_2004</th>\n",
              "      <th>Frota_2005</th>\n",
              "      <th>Frota_2006</th>\n",
              "      <th>Frota_2007</th>\n",
              "      <th>Frota_2008</th>\n",
              "      <th>Frota_2009</th>\n",
              "      <th>Frota_2010</th>\n",
              "      <th>...</th>\n",
              "      <th>Frota_2012</th>\n",
              "      <th>Frota_2013</th>\n",
              "      <th>Frota_2014</th>\n",
              "      <th>Frota_2015</th>\n",
              "      <th>Frota_2016</th>\n",
              "      <th>Frota_2017</th>\n",
              "      <th>Frota_2018</th>\n",
              "      <th>Frota_2019</th>\n",
              "      <th>Frota_2020</th>\n",
              "      <th>Frota_2021</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>148</td>\n",
              "      <td>254</td>\n",
              "      <td>380</td>\n",
              "      <td>605</td>\n",
              "      <td>847</td>\n",
              "      <td>1061</td>\n",
              "      <td>1288</td>\n",
              "      <td>1627</td>\n",
              "      <td>1974</td>\n",
              "      <td>2368</td>\n",
              "      <td>...</td>\n",
              "      <td>3187</td>\n",
              "      <td>3543</td>\n",
              "      <td>3971</td>\n",
              "      <td>4364</td>\n",
              "      <td>4648</td>\n",
              "      <td>4856</td>\n",
              "      <td>5163</td>\n",
              "      <td>5541</td>\n",
              "      <td>6045</td>\n",
              "      <td>6465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>44</td>\n",
              "      <td>65</td>\n",
              "      <td>87</td>\n",
              "      <td>115</td>\n",
              "      <td>159</td>\n",
              "      <td>236</td>\n",
              "      <td>294</td>\n",
              "      <td>366</td>\n",
              "      <td>439</td>\n",
              "      <td>525</td>\n",
              "      <td>...</td>\n",
              "      <td>795</td>\n",
              "      <td>940</td>\n",
              "      <td>1119</td>\n",
              "      <td>1271</td>\n",
              "      <td>1363</td>\n",
              "      <td>1485</td>\n",
              "      <td>1615</td>\n",
              "      <td>1700</td>\n",
              "      <td>1879</td>\n",
              "      <td>2019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>683</td>\n",
              "      <td>874</td>\n",
              "      <td>1064</td>\n",
              "      <td>1228</td>\n",
              "      <td>1468</td>\n",
              "      <td>1749</td>\n",
              "      <td>2117</td>\n",
              "      <td>2565</td>\n",
              "      <td>3095</td>\n",
              "      <td>3669</td>\n",
              "      <td>...</td>\n",
              "      <td>4737</td>\n",
              "      <td>5329</td>\n",
              "      <td>6017</td>\n",
              "      <td>6697</td>\n",
              "      <td>7235</td>\n",
              "      <td>7638</td>\n",
              "      <td>8174</td>\n",
              "      <td>8840</td>\n",
              "      <td>9450</td>\n",
              "      <td>10280</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>34</td>\n",
              "      <td>55</td>\n",
              "      <td>107</td>\n",
              "      <td>157</td>\n",
              "      <td>219</td>\n",
              "      <td>274</td>\n",
              "      <td>336</td>\n",
              "      <td>441</td>\n",
              "      <td>562</td>\n",
              "      <td>711</td>\n",
              "      <td>...</td>\n",
              "      <td>993</td>\n",
              "      <td>1146</td>\n",
              "      <td>1309</td>\n",
              "      <td>1434</td>\n",
              "      <td>1570</td>\n",
              "      <td>1652</td>\n",
              "      <td>1780</td>\n",
              "      <td>1860</td>\n",
              "      <td>2022</td>\n",
              "      <td>2172</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>36</td>\n",
              "      <td>65</td>\n",
              "      <td>98</td>\n",
              "      <td>129</td>\n",
              "      <td>171</td>\n",
              "      <td>239</td>\n",
              "      <td>348</td>\n",
              "      <td>449</td>\n",
              "      <td>567</td>\n",
              "      <td>741</td>\n",
              "      <td>...</td>\n",
              "      <td>1008</td>\n",
              "      <td>1154</td>\n",
              "      <td>1287</td>\n",
              "      <td>1468</td>\n",
              "      <td>1635</td>\n",
              "      <td>1748</td>\n",
              "      <td>1890</td>\n",
              "      <td>2003</td>\n",
              "      <td>2167</td>\n",
              "      <td>2334</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1d490ec-37d4-499e-b1a3-ad7ebcecc035')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a1d490ec-37d4-499e-b1a3-ad7ebcecc035 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a1d490ec-37d4-499e-b1a3-ad7ebcecc035');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l[\"Frota_2021\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-5_ZwF9UreEf",
        "outputId": "cba76130-7433-4229-f2aa-4970de1a6f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        6465\n",
              "1        2019\n",
              "2       10280\n",
              "3        2172\n",
              "4        2334\n",
              "        ...  \n",
              "5565    17484\n",
              "5566      490\n",
              "5567      414\n",
              "5568     2191\n",
              "5569     4729\n",
              "Name: Frota_2021, Length: 5570, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l.sum().plot(kind='bar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "85bUesqQsjoY",
        "outputId": "d7b06b2f-4ec1-43e1-8b7f-3e5ce3b5a5be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5fd88226d0>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "l.mean().plot(kind='bar')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "AO4tOuXut4Tb",
        "outputId": "6543ccf6-37d9-4584-ba58-38588db7d28d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f5fd875c810>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=l[\"Frota_2001\"], y=l[\"Frota_2021\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "id": "L88hvUvzzq-H",
        "outputId": "eaf60ec1-ffa0-46e5-a40d-fcbd9d236762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f5fd86fd0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Separar os dados em treinamento, validação e teste\n",
        "\n",
        "Serve para avaliar o modelo e evitar overfitting"
      ],
      "metadata": {
        "id": "qOZI_QiipXfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    mean_squared_error,\n",
        "    mean_absolute_error,\n",
        "    r2_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        "    mean_absolute_percentage_error\n",
        ")\n",
        "\n",
        "# O y é a coluna que queremos prever, o x é todo o resto que nos ajudará a prever\n",
        "y = l[\"Frota_2021\"]\n",
        "x = l.drop(\"Frota_2021\", axis=1)\n",
        "\n",
        "# divisão entre teste e treinamento, o random state mantém a mesma ordem\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# forma simples de pegar a validação, usando o test split de novo\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.25, random_state=1)"
      ],
      "metadata": {
        "id": "P7xAryjppXCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test e val de mesmo tamanho. train evidentemente maior\n",
        "\n",
        "len(x_train), len(y_train), len(x_test), len(y_test), len(x_val), len(y_val)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7Su4x25rYZg",
        "outputId": "5da0d901-0655-4e5a-f07f-b009e27ee5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3342, 3342, 1114, 1114, 1114, 1114)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Selecionar 4 algoritmos de regressão\n",
        "\n",
        "* Regressão Linear\n",
        "* Decision Tree\n",
        "* Random Forest\n",
        "* Multilayer perceptron"
      ],
      "metadata": {
        "id": "ZwjPHEE4sDfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLFLOW\n",
        "\n",
        "!pip install mlflow --quiet\n",
        "!pip install optuna --quiet\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import optuna"
      ],
      "metadata": {
        "id": "atpu06NKxNRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fefd9df9-65ed-44f2-a524-f56e693b101e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 16.9 MB 5.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 6.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 209 kB 66.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 79 kB 7.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 70.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 65.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 78 kB 7.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 59.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 62 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.4 MB/s \n",
            "\u001b[?25h  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 348 kB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 81 kB 5.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 112 kB 64.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 49 kB 6.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 147 kB 58.0 MB/s \n",
            "\u001b[?25h  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlflow.sklearn.autolog()\n",
        "runs = []"
      ],
      "metadata": {
        "id": "5LgMfdxlxlVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Regressão Linear\n",
        "\n",
        "Pela maneira os gráficos estão plotados, é de se imaginar que uma Regressão Linear já será suficiente para prever."
      ],
      "metadata": {
        "id": "8yEv3_Qyw0lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "LR = LinearRegression()"
      ],
      "metadata": {
        "id": "QwZu0duAZtyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run() as linear_run:\n",
        "    LR.fit(x_train, y_train)\n",
        "    pred = LR.predict(x_test)\n",
        "    \n",
        "    mse = mean_squared_error(y_test, pred)\n",
        "    mae = mean_absolute_percentage_error(y_test, pred)\n",
        "    r2 = r2_score(y_test, pred)\n",
        "\n",
        "    print(\"Mean squared error: %s\" % mse)\n",
        "    print(\"Mean absolute percentage error: %s\" % mae)\n",
        "    print(\"R2-score: %s\" % r2)\n",
        "\n",
        "    mlflow.log_metric(\"mse\", mse)\n",
        "    mlflow.log_metric(\"mae\", mae)\n",
        "    mlflow.log_metric(\"r2\", r2)\n",
        "\n",
        "    runs.append(linear_run)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN2TmHaaw21u",
        "outputId": "95fec389-2e9a-4cce-abba-8ff0039c8dad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022/10/03 14:23:37 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 2061565.8854669253\n",
            "Mean absolute percentage error: 0.018242946922896044\n",
            "R2-score: 0.9997628560850383\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LP0pXg8kbwmO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.array[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPVJPTMr5BrD",
        "outputId": "f4fadc8c-5c84-492d-93e3-cfbd2aadad08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PandasArray>\n",
              "[77114, 814, 2157, 10314, 5683, 1876, 15690, 2743, 4191, 4195]\n",
              "Length: 10, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_roJTiJv5Din",
        "outputId": "6d9303c0-efb3-4b98-daed-e80d1b3da644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([77222.71152638,   846.42641844,  2161.21608534, 10350.65759864,\n",
              "        5688.49331355,  1877.20196092, 15613.44670598,  2759.84366084,\n",
              "        4228.53661471,  4241.21286084])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(y_test.array/pred).mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-igNP645xAB",
        "outputId": "d67c8e1c-14ea-431a-a375-0db64e8991a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9895573200114399"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=y_test, y=pred)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "Zg6F6ywlu2tD",
        "outputId": "378a5e53-42eb-4f4c-a4a3-2229d58d02e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Decision tree"
      ],
      "metadata": {
        "id": "e4si07l1UOiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "T = DecisionTreeRegressor()"
      ],
      "metadata": {
        "id": "5qbTabu8UMbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run() as decisiontree_run:\n",
        "    T.fit(x_train, y_train)\n",
        "\n",
        "    pred2 = T.predict(x_test)\n",
        "\n",
        "    print(\"Mean squared error: %s\" % mean_squared_error(y_test, pred2))\n",
        "    print(\"Mean absolute percentage error: %s\" % mean_absolute_percentage_error(y_test, pred2))\n",
        "    print(\"R2-score: %s\" % r2_score(y_test, pred2))\n",
        "\n",
        "    runs.append(decisiontree_run)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqKD9NI_UYR0",
        "outputId": "047a3ba2-068f-4eab-ca62-13ab758cf924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022/10/03 14:23:40 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 156397377.6265709\n",
            "Mean absolute percentage error: 0.01980913691520391\n",
            "R2-score: 0.9820094585957422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=y_test, y=pred2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "_sGqhNsCVeLa",
        "outputId": "a006152f-200a-44ef-e512-1f0d29fcbd54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Random forest\n",
        "Por ser uma regressão simples, o R2-score foi pior, mesmo sendo um modelo mais sofisticado que Decision Trees e Linear Regression. Explicação: overfitting."
      ],
      "metadata": {
        "id": "E0M07dFv_RYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "RF = RandomForestRegressor()"
      ],
      "metadata": {
        "id": "0UtzzMeY_Qhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinando sem pruning (para ver se tem overfitting)\n",
        "RF1 = RandomForestRegressor(n_estimators=50)\n",
        "\n",
        "with mlflow.start_run() as randomforest_run:\n",
        "    RF1.fit(x_train,y_train)\n",
        "    pred3a = RF1.predict(x_test)\n",
        "\n",
        "    print(\"Mean squared error: %s\" % mean_squared_error(y_test, pred3a))\n",
        "    print(\"Mean absolute percentage error: %s\" % mean_absolute_percentage_error(y_test, pred3a))\n",
        "    print(\"R2-score: %s\" % r2_score(y_test, pred3a))\n",
        "\n",
        "    runs.append(randomforest_run)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk_35qlb-gDv",
        "outputId": "70ab8ec7-8f89-469f-ee55-61a3a5268511"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022/10/03 14:23:45 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 375527473.3099225\n",
            "Mean absolute percentage error: 0.015720667190219386\n",
            "R2-score: 0.9568027120432312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RF1.score(x_test,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLs1AV7c-s8-",
        "outputId": "a809d05b-b70c-4970-ff1a-5c14d2fe6d85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9568027120432312"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=y_test, y=pred3a)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "6FzEQHsF_HG1",
        "outputId": "063d0b77-5283-4d18-b4ae-2d8991c37242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinando com pruning (para ver se tem overfitting).\n",
        "# Para isso, utilizemos o hiperparâmetro min_samples_leaf\n",
        "RF2 = RandomForestRegressor(n_estimators=50, min_samples_leaf=25)\n",
        "\n",
        "RF2.fit(x_train,y_train)\n",
        "pred3b = RF2.predict(x_test)\n",
        "\n",
        "print(\"Mean squared error: %s\" % mean_squared_error(y_test, pred3b))\n",
        "print(\"Mean absolute percentage error: %s\" % mean_absolute_percentage_error(y_test, pred3b))\n",
        "print(\"R2-score: %s\" % r2_score(y_test, pred3b))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq07EVbpYVim",
        "outputId": "d2196683-848c-4c56-bc9a-ebadababe71c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022/10/03 14:23:48 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '0e0f0c488bb94800b54b4a743194a21d', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:23:51 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 3989935728.898609\n",
            "Mean absolute percentage error: 0.038471411123474616\n",
            "R2-score: 0.5410338394389871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sem pruning\")\n",
        "print(\"MSE Train: %s\" % mean_absolute_error(y_train, RF1.predict(x_train)))\n",
        "print(\"MSE Test: %s\" % mean_absolute_error(y_test, RF1.predict(x_test)))\n",
        "print(\"\\n\")\n",
        "print(\"Com pruning\")\n",
        "print(\"MSE Train: %s\" % mean_absolute_error(y_train, RF2.predict(x_train)))\n",
        "print(\"MSE Test: %s\" % mean_absolute_error(y_test, RF2.predict(x_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDgHg1LMZPNS",
        "outputId": "0355ee27-3157-4803-921a-5c8ef359c742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sem pruning\n",
            "MSE Train: 653.7772651107123\n",
            "MSE Test: 1124.356301615799\n",
            "\n",
            "\n",
            "Com pruning\n",
            "MSE Train: 7099.539647854278\n",
            "MSE Test: 6445.355107519119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.scatter(x=y_test, y=pred3b)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "_cri-58xd0LL",
        "outputId": "eef736ce-8aa7-44b5-cfbd-1ac2a17c2a22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nada indica que há overfitting, já que o MSE do treino e o teste são igualmente baixos sem prunning e igualmente altos com. Seria um indicativo se o MSE de teste fosse mais baixo com pruning e mais alto sem. Vê-se pelo gráfico que o primeiro generaliza melhor a função. Uma solução para se aproximar a um algoritmo tão simplório quanto a Regressão Linear é mexer nos hiperparâmetros, o que será feito mais para frente."
      ],
      "metadata": {
        "id": "2_ve96RxeEuY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. MultiLayer Perceptron\n",
        "Algoritmo com rede neural"
      ],
      "metadata": {
        "id": "4lC1hEBX_UqE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "MLP = MLPRegressor()"
      ],
      "metadata": {
        "id": "4ACg2yI-2VIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run() as mlp_run:\n",
        "    MLP.fit(x_train, y_train)\n",
        "    pred4 = MLP.predict(x_test)\n",
        "    print(\"Mean squared error: %s\" % mean_squared_error(y_test, pred4))\n",
        "    print(\"Mean absolute percentage error: %s\" % mean_absolute_percentage_error(y_test, pred4))\n",
        "    print(\"R2-score: %s\" % r2_score(y_test, pred4))\n",
        "\n",
        "    runs.append(mlp_run)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGEGX39tKIyL",
        "outputId": "4e02032e-b57c-41bc-ad52-e969f6b2bd7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022/10/03 14:23:57 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean squared error: 243615048.04721743\n",
            "Mean absolute percentage error: 0.09858395530988011\n",
            "R2-score: 0.9719767257283659\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "OBS.: como algoritmo, começamos a tentar fazer o Arima para séries temporais, mas como seriam mais de 5k colunas (as cidades) de resultado, o tempo de processamento era imenso. Seria funcional se escolhêssemos uma cidade (Recife, por exemplo) e fizéssemos a predição baseada no tempo para ela."
      ],
      "metadata": {
        "id": "Fmt7ZvypCM5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Adicionar MLFlow no treinamento\n",
        "\n",
        "Analisar o que foi colhido com o MLFLOW"
      ],
      "metadata": {
        "id": "i9HK2V6VADJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pprint import pprint"
      ],
      "metadata": {
        "id": "GjUIWJ69Yqaq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for run in runs:\n",
        "  client = mlflow.tracking.MlflowClient()\n",
        "  data = client.get_run(run.info.run_id).data\n",
        "  tags = {k: v for k, v in data.tags.items() if not k.startswith(\"mlflow.\")}\n",
        "  artifacts = [f.path for f in client.list_artifacts(run.info.run_id, \"model\")]\n",
        "\n",
        "  print(tags['estimator_name'])\n",
        "  print('\\nParameters:')\n",
        "  pprint(data.params)\n",
        "  print('\\nMetrics:')\n",
        "  pprint(data.metrics)\n",
        "  print('\\n_________________________________________\\n')\n",
        "\n",
        "print('\\nArtifacts:')\n",
        "pprint(artifacts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-H5vDkrLWqRg",
        "outputId": "99e2cf41-3efa-4424-ff58-6b6f5d67fbca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LinearRegression\n",
            "\n",
            "Parameters:\n",
            "{'copy_X': 'True',\n",
            " 'fit_intercept': 'True',\n",
            " 'n_jobs': 'None',\n",
            " 'normalize': 'deprecated',\n",
            " 'positive': 'False'}\n",
            "\n",
            "Metrics:\n",
            "{'mae': 0.018242946922896044,\n",
            " 'mse': 2061565.8854669253,\n",
            " 'r2': 0.9997628560850383,\n",
            " 'training_mae': 109.56696060495837,\n",
            " 'training_mse': 63636.34213805838,\n",
            " 'training_r2_score': 0.999997525903773,\n",
            " 'training_rmse': 252.26244694377002,\n",
            " 'training_score': 0.999997525903773}\n",
            "\n",
            "_________________________________________\n",
            "\n",
            "DecisionTreeRegressor\n",
            "\n",
            "Parameters:\n",
            "{'ccp_alpha': '0.0',\n",
            " 'criterion': 'squared_error',\n",
            " 'max_depth': 'None',\n",
            " 'max_features': 'None',\n",
            " 'max_leaf_nodes': 'None',\n",
            " 'min_impurity_decrease': '0.0',\n",
            " 'min_samples_leaf': '1',\n",
            " 'min_samples_split': '2',\n",
            " 'min_weight_fraction_leaf': '0.0',\n",
            " 'random_state': 'None',\n",
            " 'splitter': 'best'}\n",
            "\n",
            "Metrics:\n",
            "{'training_mae': 0.0,\n",
            " 'training_mse': 0.0,\n",
            " 'training_r2_score': 1.0,\n",
            " 'training_rmse': 0.0,\n",
            " 'training_score': 1.0}\n",
            "\n",
            "_________________________________________\n",
            "\n",
            "RandomForestRegressor\n",
            "\n",
            "Parameters:\n",
            "{'bootstrap': 'True',\n",
            " 'ccp_alpha': '0.0',\n",
            " 'criterion': 'squared_error',\n",
            " 'max_depth': 'None',\n",
            " 'max_features': 'auto',\n",
            " 'max_leaf_nodes': 'None',\n",
            " 'max_samples': 'None',\n",
            " 'min_impurity_decrease': '0.0',\n",
            " 'min_samples_leaf': '1',\n",
            " 'min_samples_split': '2',\n",
            " 'min_weight_fraction_leaf': '0.0',\n",
            " 'n_estimators': '50',\n",
            " 'n_jobs': 'None',\n",
            " 'oob_score': 'False',\n",
            " 'random_state': 'None',\n",
            " 'verbose': '0',\n",
            " 'warm_start': 'False'}\n",
            "\n",
            "Metrics:\n",
            "{'RandomForestRegressor_score_x_test': 0.9568027120432312,\n",
            " 'training_mae': 653.7772651107123,\n",
            " 'training_mse': 588635268.4741358,\n",
            " 'training_r2_score': 0.9771146447471784,\n",
            " 'training_rmse': 24261.806785030167,\n",
            " 'training_score': 0.9771146447471784}\n",
            "\n",
            "_________________________________________\n",
            "\n",
            "MLPRegressor\n",
            "\n",
            "Parameters:\n",
            "{'activation': 'relu',\n",
            " 'alpha': '0.0001',\n",
            " 'batch_size': 'auto',\n",
            " 'beta_1': '0.9',\n",
            " 'beta_2': '0.999',\n",
            " 'early_stopping': 'False',\n",
            " 'epsilon': '1e-08',\n",
            " 'hidden_layer_sizes': '(100,)',\n",
            " 'learning_rate': 'constant',\n",
            " 'learning_rate_init': '0.001',\n",
            " 'max_fun': '15000',\n",
            " 'max_iter': '200',\n",
            " 'momentum': '0.9',\n",
            " 'n_iter_no_change': '10',\n",
            " 'nesterovs_momentum': 'True',\n",
            " 'power_t': '0.5',\n",
            " 'random_state': 'None',\n",
            " 'shuffle': 'True',\n",
            " 'solver': 'adam',\n",
            " 'tol': '0.0001',\n",
            " 'validation_fraction': '0.1',\n",
            " 'verbose': 'False',\n",
            " 'warm_start': 'False'}\n",
            "\n",
            "Metrics:\n",
            "{'training_mae': 2980.040117817621,\n",
            " 'training_mse': 1560261539.3770223,\n",
            " 'training_r2_score': 0.9393391094140215,\n",
            " 'training_rmse': 39500.1460677935,\n",
            " 'training_score': 0.9393391094140215}\n",
            "\n",
            "_________________________________________\n",
            "\n",
            "\n",
            "Artifacts:\n",
            "['model/MLmodel',\n",
            " 'model/conda.yaml',\n",
            " 'model/model.pkl',\n",
            " 'model/python_env.yaml',\n",
            " 'model/requirements.txt']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Executar uma ferramenta de seleção de hiper-parâmetro sobre o conjunto de validação\n",
        "\n",
        "* Selecionar poucos hiper-parâmetros por algoritmo (max. 3)\n",
        "* Selecionar o modelo (algoritmo + valores de hiper-parâmetros) com melhor resultado na métrica de avaliação\n",
        "* Executar o melhor modelo de cada algoritmo no conjunto de teste e selecionar o melhor modelo geral na métrica de avaliação"
      ],
      "metadata": {
        "id": "nxydziRqsE_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Os melhores resultados de acordo com o MLFlow de MSE e R2 foram da Regressão Linear e do MultiLayer Perceptron.\n",
        "Vamos nos concentrar neles.\n",
        "Como o RL não tem hiperparâmetros para tunar, vamos apenas rodar o optuna nele; enquanto que, no MLP, entraremos manualmente com 3 valores para ser observar. Abaixo, o que o MLFlow gerou com o default:\n",
        "\n",
        "Parameters:\n",
        "{'activation': 'relu',\n",
        " 'alpha': '0.0001',\n",
        " 'batch_size': 'auto',\n",
        " 'beta_1': '0.9',\n",
        " 'beta_2': '0.999',\n",
        " 'early_stopping': 'False',\n",
        " 'epsilon': '1e-08',\n",
        " 'hidden_layer_sizes': '(100,)',\n",
        " 'learning_rate': 'constant',\n",
        " 'learning_rate_init': '0.001',\n",
        " 'max_fun': '15000',\n",
        " 'max_iter': '200',\n",
        " 'momentum': '0.9',\n",
        " 'n_iter_no_change': '10',\n",
        " 'nesterovs_momentum': 'True',\n",
        " 'power_t': '0.5',\n",
        " 'random_state': 'None',\n",
        " 'shuffle': 'True',\n",
        " 'solver': 'adam',\n",
        " 'tol': '0.0001',\n",
        " 'validation_fraction': '0.1',\n",
        " 'verbose': 'False',\n",
        " 'warm_start': 'False'}"
      ],
      "metadata": {
        "id": "hKsbO-XK8H-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_regression_optuna(trial):\n",
        "    \n",
        "    LR.fit(x_train, y_train)\n",
        "    pred = LR.predict(x_test)\n",
        "    return mean_squared_error(y_test, pred)\n",
        "\n",
        "# O tamanho das hidden_layers, onde está a rede neural de fato, é bem importante\n",
        "# para esse algoritmo. O número máximo de iterações também importa, embora possa\n",
        "# gerar overfitting. E o último que vamos mexer é o alpha, que controla\n",
        "# a regularização aplicada aos pesos da rede. É, talvez, o mais importante,\n",
        "# porque determina a minúcia da curva.\n",
        "\n",
        "def mlp_optuna(trial):\n",
        "    hls = trial.suggest_int(\"hidden_layer_sizes\", 200, 400)\n",
        "    mi = trial.suggest_int(\"max_iter\", 400, 800)\n",
        "    al = trial.suggest_float(\"alpha\", 0.00005, 0.0001)\n",
        "\n",
        "    MLP2 = MLPRegressor(hidden_layer_sizes=hls, max_iter=mi, alpha=al)\n",
        "    MLP2.fit(x_train, y_train)\n",
        "    # MLP.fit(x_train, y_train, hidden_layer_sizes=hls, max_iter=mi, alpha=al)\n",
        "    pred4 = MLP2.predict(x_test)\n",
        "    return mean_squared_error(y_test, pred4)"
      ],
      "metadata": {
        "id": "IqF-pvJhcfKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# não importa, mas aqui está\n",
        "study1 = optuna.create_study(direction=\"minimize\")\n",
        "study1.optimize(linear_regression_optuna, n_trials=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dF4bYRS0cmoE",
        "outputId": "ce6af241-69e9-419c-c5f7-d46c8f4b9177"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-03 14:25:38,553]\u001b[0m A new study created in memory with name: no-name-be806068-46f2-49b3-a8e6-d45f4cca5065\u001b[0m\n",
            "2022/10/03 14:25:38 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'f72e40eb016549cbaa08556a02a06f97', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:25:38 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:25:40,479]\u001b[0m Trial 0 finished with value: 2061565.8854669253 and parameters: {}. Best is trial 0 with value: 2061565.8854669253.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(study1.best_trial.value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ybeoh29jrML",
        "outputId": "a118498d-e30a-49a5-97b8-73b3f817bb68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2061565.8854669253\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(MLPRegressor().get_params())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SemgZXpESutQ",
        "outputId": "6a6950ae-c208-4f1f-ecc8-58a5b2d6eed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'activation': 'relu', 'alpha': 0.0001, 'batch_size': 'auto', 'beta_1': 0.9, 'beta_2': 0.999, 'early_stopping': False, 'epsilon': 1e-08, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'learning_rate_init': 0.001, 'max_fun': 15000, 'max_iter': 200, 'momentum': 0.9, 'n_iter_no_change': 10, 'nesterovs_momentum': True, 'power_t': 0.5, 'random_state': None, 'shuffle': True, 'solver': 'adam', 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': False, 'warm_start': False}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP\n",
        "study2 = optuna.create_study(direction=\"minimize\")\n",
        "study2.optimize(mlp_optuna, n_trials=50)"
      ],
      "metadata": {
        "id": "tzqzpyoHkdVC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4492a3c5-d90d-4cc4-dd80-04b2834f1714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2022-10-03 14:25:40,527]\u001b[0m A new study created in memory with name: no-name-8238d145-cc6b-493b-9b64-ff464ac48d71\u001b[0m\n",
            "2022/10/03 14:25:40 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '795ad446b8814188871f1bd10500b009', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:25:41 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:25:43,294]\u001b[0m Trial 0 finished with value: 61799300.333007514 and parameters: {'hidden_layer_sizes': 347, 'max_iter': 769, 'alpha': 9.804575965277397e-05}. Best is trial 0 with value: 61799300.333007514.\u001b[0m\n",
            "2022/10/03 14:25:43 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '50cffa83e1934f1fbd6dd7bfa48ccb2e', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:25:44 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:25:46,374]\u001b[0m Trial 1 finished with value: 25499970.510234244 and parameters: {'hidden_layer_sizes': 397, 'max_iter': 710, 'alpha': 6.12136980902154e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:25:46 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '91aaf8b721ee4174b0b7a111ee7fa76c', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:25:47 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:25:48,972]\u001b[0m Trial 2 finished with value: 299512633.42367387 and parameters: {'hidden_layer_sizes': 288, 'max_iter': 659, 'alpha': 7.930244350436724e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:25:48 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'f1dfc85aa2a74639b5321da1bd498376', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:25:49 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:25:51,486]\u001b[0m Trial 3 finished with value: 562385653.0380809 and parameters: {'hidden_layer_sizes': 264, 'max_iter': 715, 'alpha': 9.777284578328692e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:25:51 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'a0e11d7358554b24ba9e15978fc46ea4', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:25:52 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:25:53,934]\u001b[0m Trial 4 finished with value: 204892732.26544595 and parameters: {'hidden_layer_sizes': 223, 'max_iter': 609, 'alpha': 7.442796570122031e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:25:53 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'fcfdc793e5e04ef8839b51c2f62aa234', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:25:55 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:25:57,997]\u001b[0m Trial 5 finished with value: 138281075.350133 and parameters: {'hidden_layer_sizes': 386, 'max_iter': 657, 'alpha': 6.728799086690503e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:25:58 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'abe678ec609f4572b0ec258f24ee20ba', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:25:58 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:00,862]\u001b[0m Trial 6 finished with value: 88557676.93124366 and parameters: {'hidden_layer_sizes': 342, 'max_iter': 445, 'alpha': 5.0427605719819215e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:00 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'db2f41c62c5f4c1281fd0258683ea234', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:01 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:03,702]\u001b[0m Trial 7 finished with value: 250385459.91028807 and parameters: {'hidden_layer_sizes': 215, 'max_iter': 494, 'alpha': 8.99307353229864e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:03 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'd53f8052578042e4bf849970e2268862', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:04 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:06,081]\u001b[0m Trial 8 finished with value: 35389056.270562015 and parameters: {'hidden_layer_sizes': 238, 'max_iter': 414, 'alpha': 7.95021401325628e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:06 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '284fd2f31d0c4c4088e3c1f9e70201e4', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:06 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:08,673]\u001b[0m Trial 9 finished with value: 113492511.84937257 and parameters: {'hidden_layer_sizes': 357, 'max_iter': 723, 'alpha': 7.794878866516951e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:08 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'af3cc235e2e34f308e9a4625e4d96b9a', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:10 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:12,126]\u001b[0m Trial 10 finished with value: 204667538.6924919 and parameters: {'hidden_layer_sizes': 397, 'max_iter': 526, 'alpha': 5.4842672799949583e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:12 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '0b27f75930524eea948ff6602933bb97', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:12 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:14,710]\u001b[0m Trial 11 finished with value: 85634052.97441845 and parameters: {'hidden_layer_sizes': 258, 'max_iter': 405, 'alpha': 6.220764007984023e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:14 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '6f6ba78fbeb4493198c3ecc9ff7a3cc5', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:16 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:18,653]\u001b[0m Trial 12 finished with value: 835836133.6704715 and parameters: {'hidden_layer_sizes': 313, 'max_iter': 559, 'alpha': 6.667860854986056e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:18 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'ec700880c6184f369e6ea55ab1d9815f', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:19 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:21,604]\u001b[0m Trial 13 finished with value: 150916371.2231704 and parameters: {'hidden_layer_sizes': 242, 'max_iter': 789, 'alpha': 8.543001386016248e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:21 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '8d6c25d1c42447a2bbf853cb210ecef8', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:22 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:24,294]\u001b[0m Trial 14 finished with value: 330621161.6284552 and parameters: {'hidden_layer_sizes': 303, 'max_iter': 609, 'alpha': 6.0666057206350434e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:24 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'deddd30c3cad4475a844d22d07af2353', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:25 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:27,113]\u001b[0m Trial 15 finished with value: 56190007.61652488 and parameters: {'hidden_layer_sizes': 200, 'max_iter': 672, 'alpha': 7.03260792936003e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:27 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '86a9f931565343d1843e9c96f8ac18a9', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:28 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:30,067]\u001b[0m Trial 16 finished with value: 89248886.98569965 and parameters: {'hidden_layer_sizes': 274, 'max_iter': 458, 'alpha': 8.51633156527298e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:30 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '218dfa5c7bec4a3d8c0e3ac9a829a9bb', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:30 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:32,701]\u001b[0m Trial 17 finished with value: 689003407.2377417 and parameters: {'hidden_layer_sizes': 322, 'max_iter': 538, 'alpha': 6.006150209504873e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:32 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'cdbf5dc9e84f4616b1c432b840f79d9a', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:33 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:35,639]\u001b[0m Trial 18 finished with value: 53059437.36979098 and parameters: {'hidden_layer_sizes': 371, 'max_iter': 738, 'alpha': 7.263223880854032e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:35 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'ffe904c2e28943458e7fab67878bd0c5', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:36 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:38,486]\u001b[0m Trial 19 finished with value: 71894788.81540543 and parameters: {'hidden_layer_sizes': 235, 'max_iter': 572, 'alpha': 8.148860763442775e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:38 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '65ae0189e1e948acb27509b80417cb16', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:39 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:41,344]\u001b[0m Trial 20 finished with value: 148243676.90031758 and parameters: {'hidden_layer_sizes': 333, 'max_iter': 403, 'alpha': 9.109989909488806e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:41 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '8f99890c06c5439d91014c263bc3ce28', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:42 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:43,950]\u001b[0m Trial 21 finished with value: 52180899.13744962 and parameters: {'hidden_layer_sizes': 374, 'max_iter': 753, 'alpha': 7.303680522667077e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:43 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '2a61c629c307494f864debfb6881f721', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:46 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:48,111]\u001b[0m Trial 22 finished with value: 50510188.17833348 and parameters: {'hidden_layer_sizes': 377, 'max_iter': 692, 'alpha': 6.64038143994766e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:48 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '30faaa9985bf4138a96f0942f06463ef', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:48 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:50,780]\u001b[0m Trial 23 finished with value: 1175911167.4095001 and parameters: {'hidden_layer_sizes': 400, 'max_iter': 698, 'alpha': 6.57863298404103e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:50 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '07128276e244445ebfa6b25a3ca718a0', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:51 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:53,792]\u001b[0m Trial 24 finished with value: 233166983.27488592 and parameters: {'hidden_layer_sizes': 363, 'max_iter': 635, 'alpha': 5.588979110398818e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:53 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '022e07b95729454f8e536bf7293566d3', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:54 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:56,592]\u001b[0m Trial 25 finished with value: 989895832.2214402 and parameters: {'hidden_layer_sizes': 288, 'max_iter': 696, 'alpha': 6.41054344906449e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:56 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '8a7318fdb3894bd199ab5fe776a74531', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:26:57 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:26:59,549]\u001b[0m Trial 26 finished with value: 245736155.27701348 and parameters: {'hidden_layer_sizes': 383, 'max_iter': 792, 'alpha': 7.028920214954868e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:26:59 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'dd7d4f5ea31240818e1144facfa36358', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:00 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:02,459]\u001b[0m Trial 27 finished with value: 1713966507.715899 and parameters: {'hidden_layer_sizes': 355, 'max_iter': 624, 'alpha': 5.5380175562359716e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:02 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'c5aea286da884fd2b03664ac79090f75', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:03 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:05,007]\u001b[0m Trial 28 finished with value: 63763590.23405705 and parameters: {'hidden_layer_sizes': 252, 'max_iter': 676, 'alpha': 5.8673355596008794e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:05 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '515bc2de2f6f4ebcaa96d1268120550d', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:06 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:07,908]\u001b[0m Trial 29 finished with value: 70306424.77479157 and parameters: {'hidden_layer_sizes': 335, 'max_iter': 755, 'alpha': 5.0553641253971833e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:07 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'cf43e0255ad94b1394d384116db59d54', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:08 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:10,757]\u001b[0m Trial 30 finished with value: 228181014.70652443 and parameters: {'hidden_layer_sizes': 387, 'max_iter': 589, 'alpha': 7.704621360984037e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:10 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '0ca6294d744148beb9fa6b9413a9396c', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:12 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:13,924]\u001b[0m Trial 31 finished with value: 72325492.26999784 and parameters: {'hidden_layer_sizes': 370, 'max_iter': 762, 'alpha': 7.039950651440585e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:13 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '07e37dbaab5d4bbf978ca124f35317cc', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:14 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:16,819]\u001b[0m Trial 32 finished with value: 175639845.48597226 and parameters: {'hidden_layer_sizes': 382, 'max_iter': 735, 'alpha': 8.255338096015518e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:16 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'd2d381196f554049bc4cce123b45614d', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:18 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:19,922]\u001b[0m Trial 33 finished with value: 593544129.2529464 and parameters: {'hidden_layer_sizes': 349, 'max_iter': 711, 'alpha': 7.390523338701575e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:19 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'c6b3f78c72a14531a53df452169ad668', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:20 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:22,795]\u001b[0m Trial 34 finished with value: 105098500.84370513 and parameters: {'hidden_layer_sizes': 377, 'max_iter': 642, 'alpha': 6.802554542512734e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:22 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '19fc628599ec47e8ad6acabd104fc373', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:23 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:25,436]\u001b[0m Trial 35 finished with value: 146357216.22126216 and parameters: {'hidden_layer_sizes': 400, 'max_iter': 684, 'alpha': 7.641792026282719e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:25 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'b6e6176bd3c2497592baa8ef0c9423bb', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:28 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:30,399]\u001b[0m Trial 36 finished with value: 250693584.66835007 and parameters: {'hidden_layer_sizes': 287, 'max_iter': 767, 'alpha': 8.020579462778e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:30 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'edc6797b79e046d0ab494b3f55f93a90', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:31 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:33,793]\u001b[0m Trial 37 finished with value: 105602200.57031922 and parameters: {'hidden_layer_sizes': 364, 'max_iter': 656, 'alpha': 6.380306482957094e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:33 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'f9dce110060043079990835eb1b8b6aa', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:34 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:36,567]\u001b[0m Trial 38 finished with value: 266152184.70575538 and parameters: {'hidden_layer_sizes': 344, 'max_iter': 746, 'alpha': 9.764099349160118e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:36 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '5554ef71d2724cc89bfb4e3d9af5bc86', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:37 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:39,462]\u001b[0m Trial 39 finished with value: 112308135.84800678 and parameters: {'hidden_layer_sizes': 391, 'max_iter': 713, 'alpha': 7.338744705355897e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:39 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'ba750f4c86604e5fa2f11e279c37c521', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:41 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:43,193]\u001b[0m Trial 40 finished with value: 104432680.6972659 and parameters: {'hidden_layer_sizes': 271, 'max_iter': 500, 'alpha': 6.95015169761967e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:43 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'aa0d45a120d943e48fe24eb0794e25cc', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:44 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:46,478]\u001b[0m Trial 41 finished with value: 104560438.75432253 and parameters: {'hidden_layer_sizes': 376, 'max_iter': 727, 'alpha': 7.239490971114619e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:46 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '4cc39235700e4bc38e599025ad2cb673', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:47 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:49,560]\u001b[0m Trial 42 finished with value: 212599529.90760636 and parameters: {'hidden_layer_sizes': 374, 'max_iter': 783, 'alpha': 7.872754924910961e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:49 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '9a24bf554d704fad91fa00c7c65054c1', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:51 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:53,291]\u001b[0m Trial 43 finished with value: 91195998.60120328 and parameters: {'hidden_layer_sizes': 390, 'max_iter': 740, 'alpha': 7.47427020862035e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:53 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '01d04f9088d3493598788dafb44e6a62', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:54 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:56,189]\u001b[0m Trial 44 finished with value: 641307423.5870254 and parameters: {'hidden_layer_sizes': 362, 'max_iter': 433, 'alpha': 7.221741126517396e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:56 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '7c4c72ee97074939b662aee0884a95e4', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:56 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:27:58,787]\u001b[0m Trial 45 finished with value: 66067321.373634994 and parameters: {'hidden_layer_sizes': 368, 'max_iter': 776, 'alpha': 6.412111695740191e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:27:58 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '0f6bc186712343b2bd20bd515bd90c6f', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:27:59 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:28:01,421]\u001b[0m Trial 46 finished with value: 91584824.66456257 and parameters: {'hidden_layer_sizes': 218, 'max_iter': 697, 'alpha': 7.652644592559981e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:28:01 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '9d6221fa30a44e56ab3522254dc497d2', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:28:02 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:28:04,454]\u001b[0m Trial 47 finished with value: 78040229.97020663 and parameters: {'hidden_layer_sizes': 352, 'max_iter': 728, 'alpha': 8.378039405000721e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:28:04 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'bc7ab93aaac64a32a7616862912bdc56', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:28:05 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:28:07,175]\u001b[0m Trial 48 finished with value: 71506266.57239676 and parameters: {'hidden_layer_sizes': 326, 'max_iter': 475, 'alpha': 6.795572081872957e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n",
            "2022/10/03 14:28:07 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'a07889d8f8634cee9c820de6171cfbd5', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
            "2022/10/03 14:28:07 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "\u001b[32m[I 2022-10-03 14:28:09,838]\u001b[0m Trial 49 finished with value: 492215098.6062004 and parameters: {'hidden_layer_sizes': 307, 'max_iter': 660, 'alpha': 8.953138428364616e-05}. Best is trial 1 with value: 25499970.510234244.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(study2.best_trial.value)\n",
        "print(study2.best_trial.params)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5jTw9hc0x_K",
        "outputId": "d6b549c5-362d-4a43-8c6b-55c83b8b6a11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25499970.510234244\n",
            "{'hidden_layer_sizes': 397, 'max_iter': 710, 'alpha': 6.12136980902154e-05}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Realizar diagnóstico do melhor modelo geral da etapa 5 e melhorá-lo a partir do diagnóstico"
      ],
      "metadata": {
        "id": "7sCuECS-E4Ml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos realizar o diagnóstico no nosso modelo de Regressão Linear. Como ele é um modelo simples, vamos verificar se o mesmo não está com Underfitting, verificando os erros no conjunto de teste e treinamento para verificar se temos um alto viés.\n",
        "\n",
        "O ideal seria termos erros até de 1%.\n"
      ],
      "metadata": {
        "id": "hCVOAXO9E4Mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test.array[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yffmz-gzNmo_",
        "outputId": "a1b6280d-986d-4056-f11d-a4ecfcfb9a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PandasArray>\n",
              "[77114, 814, 2157, 10314, 5683, 1876, 15690, 2743, 4191, 4195]\n",
              "Length: 10, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OprpAboaOhx-",
        "outputId": "166ce9d8-6224-43c2-bbc2-418ed62f7768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([77222.71152638,   846.42641844,  2161.21608534, 10350.65759864,\n",
              "        5688.49331355,  1877.20196092, 15613.44670598,  2759.84366084,\n",
              "        4228.53661471,  4241.21286084])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.array[0:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bG7R6tJPQS-Q",
        "outputId": "169bd41f-cc98-44f8-ac07-9179aa5cfabd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PandasArray>\n",
              "[2403, 49491, 1233, 8839, 60403, 1600, 25735, 8288, 4275, 1224]\n",
              "Length: 10, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Test_Error = mean_absolute_percentage_error(y_test, pred)\n",
        "print(\"Test Error: %s\" % Test_Error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soRfG9ycSfjN",
        "outputId": "0a0e8e63-d930-4958-d992-8d00da56049a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Error: 0.018242946922896044\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run() as linear_run:\n",
        "    LR.fit(x_train, y_train)\n",
        "    pred = LR.predict(x_train)\n",
        "    \n",
        "    Train_Error = mean_absolute_percentage_error(y_train, pred)\n",
        "    print(\"Train Error: %s\" % Train_Error)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlnRnSEvS89n",
        "outputId": "c2bbb7ef-3d6a-432c-b3fe-bdf92791daef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022/10/03 14:45:11 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.7/dist-packages/mlflow/models/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Error: 0.02187898342109102\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Temos um erro de 1,82% para o conjunto de teste e um erro de 2,18% para o conjunto de treinamento. \n",
        "\n",
        "Com um viés de 1,18% e uma variância de 0,36%. Sendo assim, essa baixa variância e baixo viés não caracterizam Underfiting. Portanto, não há necessidade de melhora-lo"
      ],
      "metadata": {
        "id": "IfYn7Zp7E4Mu"
      }
    }
  ]
}
